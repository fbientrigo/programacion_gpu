
<!DOCTYPE html>
<html>
<head>

<meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="chrome=1" />

<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />


<title>Execution model</title>

<!-- General and theme style sheets -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.9.0/css/reveal.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.9.0/css/theme/white.css" id="theme">
<!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.9.0/lib/css/zenburn.css"> -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.2.0/styles/sunburst.min.css">


        <style type="text/css">
            .container{
                        display: flex;
                      }
            .col{
                      flex: 1;
                }
            .reveal section p {
                      display: inline-block;
                      font-size: 0.6em;
                      line-height: 1.2em;
                      vertical-align: top;
                      text-align: left;
            }
            .reveal section li {
                      font-size: 0.6em;
            }
            .reveal section td {
                      font-size: 0.6em;
            }
            .reveal section img {
                      border: none;
            }
        </style>

	</head>
	<body>
		<div class="reveal">
			<div class="slides">

                <section><h2>CUDA execution model</h2>
                </section>

				<section><h3>GPU Architecture Overview</h3>
                <p>Architecture is built around an array of <i>Streaming Multiprocessors</i> (SM).</p>
                <div class="container">
                <div class="column">
                <p>Key components:</p>
                <ul><li>CUDA cores</li>
                    <li>Shared memory/L1 cache</li>
                    <li>Register file</li>
                    <li>Load/store units</li>
                    <li>Special function units</li>
                    <li>Warp scheduler</li></ul>
                </div>
                <div class="column">
                <img src="execution_model_figs/figure_3_1.png" height=400>
                <p>Fermi architecture</p>
                </div>
                </div>
                </section>

                <section><h4>GPU Architecture Overview</h4>
                <p><ul><li>Each SM supports concurrent execution of hundreds of threads.</li>
                    <li>When a kernel is launched, thread blocks are distributed among available SMs.</li>
                    <li>On each SM, blocks are partitioned into groups of 32 threads called <i>warps</i>.</li>
                    <li>All threads in a warp execute the same instruction at the same time.</li>
                    <li>Each thread has its own instruction address counter and register state.</li></ul></p>
                </section>

                <section><h4>Single-Instruction-Mutliple-Thread (SIMT)</h4>
                <p><ul><li>SIMD model is used for multithreading on CPU.</li>
                    <li>SIMD requires that all vector elements in a vector execute together in a unified synchronous group.</li>
                    <li>With SIMT, multiple threads in the same warp may execute independently, thus individual threads may have different behaviour.</li></ul></p>
                <p>Thus in SIMT threads have 3 important aspects that SIMD threads do not:</p>
                <p><ul><li>Each thread has its own instruction address counter.</li>
                    <li>Each thread has its own register state.</li>
                    <li>Each thread can have an independent execution path.</li></ul></p>
                </section>
 
                <section><h4>GPU Architecture Overview</h4>
                <p>A thread block is schedules on one SM and remains there until execution completes.</p>
                <figure>
                <img src="execution_model_figs/figure_3_2.png" height=400>
                <figcaption style="font-size: 18px">Logical/hardware view of CUDA</figcaption>
                </figure>
                </section>

                <section><h4>GPU Architecture Overview</h4>
                <p><ul><li>Registers are partitioned among threads and shared memory is partitioned among blocks.</li>
                       <li>Threads in a thread block can communicate through these resources.</li>
                       <li>Threads within a block logically run in parallel, but physically they cannot all execute at the same time.</li>
                       <li>Sharing data among threads can lead to <i>race conditions</i>.</li>
                       <li>Synchronisation of all threads in a block is possible using primitives.</li>
                       <li>Inter-block synchronisation is now possible using <i>cooperative groups</i> (added in CUDA 9).</li></ul></p>
                </section>

				<section><h4>GPU Volta Architecture</h4>
                <a href="https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf">Volta White Paper</a>
                </section>         

				<section><h4>Profile-Driven Optimisation</h4>
                <p><i>Profiling</i> is the act of analysing program performance by measuring:</p>
                <p><ul><li>Memory or compute complexity of the code</li>
                       <li>Use of particular instructions</li>
                       <li>Frequency and duration of function calls</li></ul></p>
                <p>In HPC applications the sequence of code development is usually:</p>
                <p><ol><li>Develop the code and ensure the results are correct</li>
                       <li>Optimise the code to improve performance (profiling)</li></ol></p>
                </section>

                <section><h4>Profile-Driven Optimisation</h4>
                <p>Two main profiling tools: <code class="language-c">nvvp</code>, a visual profiler; and <code class="language-c">nvprof</code>, a command-line profiler.</p>
                <p>These profilers collect information about <i>events</i> and <i>metrics</i>:</p>
                <p><ul><li>An event is a countable activity corresponding to a hardware counter collected during kernel execution.</li>
                       <li>A metric is calculated from one or more events.</li>
                       <li>Most counters are reported per SM, not the whole GPU.</li>
                       <li>A single run will collect only a few counters, and some are mutually exclusive. Several runs may be required to get full information.</li>
                       <li>Counter values may vary between runs.</li></ul></p>
                </section>

                <section><h4>Profile-Driven Optimisation</h4>
                <p><span style="color: red">Important note</span>: for devices of compute capability 7.2 or higher <code class="language-c">nvprof</code> no longer works!</p>
                <p>The Nsight Compute Command-Line Interface (CLI) may be used instead: <code class="language-c">ncu</code>.</p>
                </section>

                <section><h4>Profile-Driven Optimisation</h4>
                <p>Main performance limiters:</p>
                <p><ul><li>Memory bandwidth</li>
                       <li>Compute resources</li>
                       <li>Instruction and memory latency</li></ul></p>
                <p>This chapter concentrates on instruction latency and some aspects of compute resource limitations.</p>
                </section>

                <section><h3>Warp execution</h3>
                <p><ul><li>Kernel is launched, thread blocks are distributed among SMs.</li>
                       <li>Then each block is partitioned into warps.</li>
                       <li>All 32 threads in a warp execute the same instruction on its own private data (SIMT model).</p>
                <img src="execution_model_figs/figure_3_10.png">
                </section>

                <section><h4>Warp execution</h4>
                <p>Thread blocks are 1-, 2- or 3-dimensional logically. In hardware, everything is one dimensional.</p>
                <p>As an example, 1D block with $128$ threads:</p>
                <p><ul><li>Warp 0: threads 0 to 31</li>
                       <li>Warp 1: threads 32 to 63</li>
                       <li>Warp 2: threads 64 to 95</li>
                       <li>Warp 3: threads 96 to 127</li></ul></p>
                <p>For a 3D block the unique thread ID is:</p>
                <p><code class="language-c">threadIdx.z*blockDim.y*blockDim.x + threadIdx.y*blockDim.x + threadIdx.x</code></p>
                </section>

                <section><h4>Warp execution</h4>
                <p>The number of warps per block is given by $\text{ceil} ( \text{threads per block} / \text{warp size} )$.</p>
                <p>A warp is never split between blocks.<p>
                <figure>
                <img src="execution_model_figs/figure_3_11.png">
                </figure>
                </section>

                <section><h3>Warp divergence</h3>
                <p><ul><li>Conditionals (if/else) lead to code <i>branching</i>.</li>
                       <li>CPUs have hardware for <i>branch prediction</i>: attempting to predict which branch will be executed.</li>
                       <li>If the prediction is correct the performance cost of branching is substantially reduced.</li>
                       <li>GPUs do not have branch prediction, and all threads in a warp must execute the <i>same</i> instruction.</li></ul></p>
                </section>

                <section><h4>Warp divergence</h4>
                <p>Consider a typical if/else construction:</p>
                <p><pre><code class="language-c">if (cond) {
    ...
} else {
    ...
}</code></pre></p>
                <p>Example: for $16$ threads in a warp <code class="language-c">cond</code> is <code class="language-c">true</code>, while for the other $16$ it is <code class="language-c">false</code>.</p>
                <p>This means half the threads in the warp execute the <code class="language-c">if</code> block and half execute the <code class="language-c">else</code> block. But... all threads in a warp must execute the same instruction!</p>
                </section>

                <section><h4>Warp divergence</h4>
                <p><ul><li>This circumstance is known as <i>warp divergence</i>.</li>
                       <li>The warp executes each branch sequentially, with half of the threads disabled.</li>
                       <li>This can severely degrade performance.</li></ul></p>
                <img src="execution_model_figs/figure_3_12.png">
                </section>

                <section><h4>Warp divergence</h4>
                <div class="container">
                <div class="column">
                <pre><code class="language-c">__global__ void mathKernel1(float *c) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    float a, b;
    a = b = 0.0f;

    if (tid % 2 == 0) {
        a = 100.0f;
    } else {
        b = 200.0f;
    }

    c[tid] = a + b;
}</pre></code>
                </div>
                <div class="column">
                <p>In this case, even-numbered threads execute the <code class="language-c">if</code> block and odd-numbered threads execute the <code class="language-c">else</code> block.</p>
                <p>This (maximal) warp divergence can be avoided by working at the level of warps.</p>
                </div>
                </section>

                <section><h4>Warp divergence</h4>
                <div class="container">
                <div class="column">
                <pre><code class="language-c">__global__ void mathKernel2(float *c) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    float a, b;
    a = b = 0.0f;

    if ((tid / warpSize) % 2 == 0) {
        a = 100.0f;
    } else {
        b = 200.0f;
    }

    c[tid] = a + b;
}</pre></code>
                </div>
                <div class="column">
                <p>Now it is threads in even-numbered <i>warps</i> that execute the <code class="language-c">if</code> block and odd-numbered warps that execute the <code class="language-c">else</code> block.</p>
                <p>There is no warp divergence in this case.</p>
                </div>
                </div>
                </section>

                <section><h4>Warp divergence</h4>
                <p><ul><li>We can compare these kernels in the code <code class="language-c">simpleDivergence.cu</code>.</li>
                       <li>The profiler can give us more information:</li></ul></p> 
                           <p><code class="language-c">nvprof --metrics branch_efficiency ./simpleDivergence</code></p>
                <p>Important: for my GPU the compilation must use <code class="language-c">-arch=sm_50</code> otherwise the code compiles but the kernels are never run! The profiler then reports that no kernels were profiled. For some metrics it may be necessary to run <code class="language-c">nvprof</code> using <code class="language-c">sudo</code>.</p>
                </section>

                <section><h4>Warp divergence</h4>
                <p>Branch efficiency is defined as: $100 \times \left( \frac{\text{# branches} - \text{# divergent branches}}{\text{# branches}} \right)$</p>
                <p>In our code branch efficiency is $100$%. This is due to compiler optimisations which replace branch instructions with <i>predicated</i> instructions for short, conditional code segments...</p>
                </section>

                <section><h4>Warp divergence</h4>
                <p><pre><code class="language-c">__global__ void mathKernel3(float *c) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    float ia, ib;
    ia = ib = 0.0f;

    bool ipred = (tid % 2 == 0);
    if (ipred) {
        ia = 100.0f;
    }
    if (!ipred) {
        ib = 200.0f;
    }
    c[tid] = ia + ib;
}</code></pre></p>
                </section>

                <section><h4>Warp divergence</h4>
                <p>This compiler optimisation removes the warp divergence, but is only applied for short if/else blocks.</p>
                <p>We can force the compiler NOT to optimise the branches in this way:</p>
                <p><code class="language-c">nvcc -g -G -arch=sm_50 simpleDivergence.cu -o simpleDivergence.x</code></p>
                <p>We can also find the number of branches and divergent branches using nvprof:</p>
                <p><code class="language-c">nvprof --events branch,divergent_branch ./simpleDivergence.x</code></p>
                </section>

                <section><h3>Resource partitioning</h3>
                <p>Each thread has access to the following resources:</p>
                <p><ul><li>Program counters</li>
                       <li>Registers</li>
                       <li>Shared memory</li></ul></p>
                <p>Each SM has 32-bit registers that are partitioned among threads.</p>
                <p>The shared memory is partitioned among thread blocks.</p>
                <p>The number of blocks and warps that can reside on an SM for a given kernel depends on what the kernel requires and the resources available.</p>
                </section>

                <section><h4>Resource partitioning</h4>
                <p><img src="execution_model_figs/figure_3_13.png"></p>
                <p><img src="execution_model_figs/figure_3_14.png"></p>
                </section>

                <section><h4>Resource partitioning</h4>
                <p>If there are insufficient resources to allocate at least one block on an SM, the kernel launch will fail.</p>
                <p><img src="execution_model_figs/table_resources.png"></p>
                </section>

                <section><h4>Resource partitioning</h4>
                <p><ul><li><i>Active block</i>: block that has been allocated resources.</li>
                       <li><i>Active warp</i>: warps within an active block.</li>
                       <ul><li><i>Selected warp</i>: actively executing warp.</li>
                           <li><i>Stalled warp</i>: not ready for execution.</li>
                           <li><i>Eligible warp</i>: 32 cores available, all instruction arguments ready.</li></ul></ul></p>
                </section>

                <section><h3>Latency hiding</h3>
                <p>The number of clock cycles between an instruction being issued and an instruction being completed is <i>instruction latency</i>.</p>
                <p>Full compute resource utilisation is achieved when all warp schedulers have an eligible warp at every clock cycle.</p>
                <p>CPUs are designed to <b>minimise latency</b> for a few threads.</p>
                <p>GPUs <b>maximise throughput</b> to hide latency.</p>
                </section>

                <section><h4>Latency hiding</h4>
                <p>Instruction latency:</p>
                <p><ul><li>Arithmetic instructions: 10-20 cycles</li>
                       <li>Global memory access instructions: 400-800 cycles</li></ul></p>
                <img src="execution_model_figs/figure_3_15.png">
                </section>

                <section><h4>Latency hiding</h4>
                <p>We can estimate the number of active warps required to hide latency:</p>
                <p>Number of required warps $=$ Latency $\times$ Throughput</p>
                <img src="execution_model_figs/figure_3_16.png">
                </section>

                <section><h4>Latency hiding</h4>
                <img src="execution_model_figs/throughput_and_bandwidth.png">
                </section>

                <section><h4>Latency hiding: arithmetic</h4>
                <img src="execution_model_figs/table_3_3.png">
                <p>Fermi GPUs require 20 warps per SM for full compute utilisation.</p>
                <p>Two ways to increase parallelism:</p>
                <p><ul><li>Instruction-level parallelism: more independent instructions within a thread</li>
                       <li>Thread-level parallelism: more concurrently eligible threads</li></ul></p>
                </section>

                <section><h4>Latency hiding: memory</h4>
                <img src="execution_model_figs/table_3_4.png">
                <p>Device memory frequency can be checked with:</p>
                <p><code class="language-c">nvidia-smi -a -q -d CLOCK | fgrep -A 3 "Max Clocks" | fgrep "Memory"</code></p>
                </section>

                <section><h4>Latency hiding: memory</h4>
                <p>Example with Fermi architecture:</p>
                <p>Memory frequency of 1.566 GHz, bandwidth 144 GB/s: (144 GB/s)/1.566 = 92 Bytes/cycle</p>
                <p>Multiplying by memory latency of 800 cycles, we need 74 KB of memory "in-flight" for full utilisation.</p>
                <p>If each thread moves one <code class="language-c">float</code> (4 bytes) from global memory, we need 18500 threads (579 warps) to hide memory latency.</p>
                </section>

                <section><h4>Latency hiding</h4>
                <p>Latency hiding depends on the number of active warps per SM.</p>
                <p>This is determined by the execution configuration and the resource constraints (registers, shared memory)</p>
                <p>Finding an optimal execution configuration requires finding the balance between latency hiding and resource utilisation.</p>
                </section>

                <section><h4>Occupancy</h4>
                <p>Instructions are sequential within each CUDA core.</p>
                <p>When a warp stalls, the SM switches to other eligible warps.</p>
                <p>Ideally, we should have enough warps to keep the cores of the GPU occupied.</p>
                <p><i>Occupancy</i>: active warps / maximum warps (per SM)</p>
                <p>The maximum number of warps per SM may be found from <code class="language-c">cudaGetDeviceProperties</code> (using <code class="language-c">maxThreadsPerMultiProcessor</code> property).</p>
                <p>See <code class="language-c">simpleDeviceQuery.cu</code></p>
                </section>

                <section><h4>Occupancy</h4>
                <p>CUDA Toolkit includes an Excel spreadsheet called the <i>CUDA Occupancy Calculator</i> to assist in selecting grid/block sizes to maxmimise occpancy for a kernel.</p>
                <img src="execution_model_figs/spreadsheet.png" height=400>
                </section>

                <section><h4>Occupancy</h4>
                <p>Calculation requires information about resource usage:</p>
                <p><ul><li>Threads per block</li>
                       <li>Registers per thread (can be found from compiler using flag <code class="language-c">--ptxas-options=-v</code>)</li>
                       <li>Shared memory per block (see later in the course)</li></ul></p>
                <p>Register usage can be manually controlled using <code class="language-c">-maxrregcount=NUM</code> in the compiler. This limits the number of registers per thread to <code class="language-c">NUM</code>.</p>
                </section>

                <section><h4>Occupancy</h4>
                <p>Occupancy can be significantly affected by block configuration:</p>
                <p><ul><li><b>Small thread blocks:</b> too few threads per block causes the maximum number of warps on an SM to be reached before resources are fully utilised.</li>
                       <li><b>Large thread blocks:</b> too many threads per block leads to fewer resources available (on each SM) for each thread.</li></ul></p>
                </section>

                <section><h4>Occupancy: guidelines</h4>
                <p><img src="execution_model_figs/guidelines_grid_block.png"></p>
                </section>

                <section><h3>Synchronisation</h3>
                <p>Synchronisation is possible at two levels (without the use of <i>cooperative groups</i>):</p>
                <p><ul><li><b>System-level:</b> synchronise host and device:</li></ul></p>
                <p><code class="language-c">cudaError_t cudaDeviceSynchronize(void);</code></p>
                <p><ul><li><b>Block-level:</b> wait until all threads in a block reach the same point:</li></ul></p>
                <p><code class="language-c">__device__ void __syncthreads(void);</code></p>
                </section>

                <section><h4>Synchronisation and scalability</h4>
                <p>Block synchronisation is prohibited to ensure <i>scalability</i>.</p>
                <p>Thread blocks may execute in any order, in parallel or in series.</p>
                <p>This flexibility ensures that CUDA programs are scalable across an arbitrary number of compute cores (i.e. it should run faster on a more powerful GPU without any major modification).</p>
                </section>

                <section><h4>Synchronisation and scalability</h4>
                <p><img src="execution_model_figs/figure_3_18.png"></p>
                </section>

                <section><h3>Practical example: profiling <code class="language-c">sumMatrixOnGPU2D</code></h3>
                <p>We modify this code to allow the block configuration to be specified on the command line:</p>
                <p><pre><code class="language-c">if (argc > 2) {
    dimx = atoi(argv[1]);
    dimy = atoi(argv[2]);
}
dim3 block(dimx, dimy);
dim3 grid((nx + bloxk.x - 1) / block.c, (ny + block.y - 1) / block.y);</code></pre></p>
                </section>

                <section><h4>Practical example: profiling <code class="language-c">sumMatrixOnGPU2D</code></h4>
                <p>Now we set a baseline:</p>
                <p><table><tr><td>Configuration</td><td>Execution time (ms)</td></tr>
                          <tr><td>$(32,32)$    </td><td>      $51.8$       </td></tr>
                          <tr><td>$(32,16)$    </td><td>      $46.1$       </td></tr>
                          <tr><td>$(16,32)$    </td><td>      $46.7$       </td></tr>
                          <tr><td>$(16,16)$    </td><td>      $47.5$       </td></tr></table></p>
                <p>We can now confirm if these differences are due to differences in occupancy by using:</p>
                <p><code class="language-c">nvprof --metrics achieved_occupancy</code></p> 
                <p>(On Kosmos it should be <code class="language-c">ncu</code>.)</p>
                </section>

                <section><h4>Practical example: profiling <code class="language-c">sumMatrixOnGPU2D</code></h4>
                <p><table><tr><td>Configuration</td><td>Execution time (ms)</td><td>Achieved occupancy</td></tr>
                          <tr><td>$(32,32)$    </td><td>      $51.8$       </td><td>$81.4\%$          </td></tr>
                          <tr><td>$(32,16)$    </td><td>      $46.1$       </td><td>$86.3\%$</td></tr>
                          <tr><td>$(16,32)$    </td><td>      $46.7$       </td><td>$83.8\%$</td></tr>
                          <tr><td>$(16,16)$    </td><td>      $47.5$       </td><td>$87.9\%$</td></tr></table></p>
                <p>Second run has higher occupancy than the first, thus it is faster.</p>
                <p>Last run has highest occupancy but is not the fastest: there are other factors restricting performance.</p>
                </section>

                <section><h4>Practical example: profiling <code class="language-c">sumMatrixOnGPU2D</code></h4>
                <p>There are 3 memory operations in the kernel: <code class="language-c">C[idx] = A[idx] + B[idx]</code> (2 loads and 1 store).</p>
                <p>We can check memory throughput with <code class="language-c">nvprof --metrics gld_throughput</code>.</p>
                <p><table><tr><td>Configuration</td><td>Execution time (ms)</td><td>Global load throughput</td></tr>
                          <tr><td>$(32,32)$    </td><td>      $51.8$       </td><td>$2.53$ GB/s</td></tr>
                          <tr><td>$(32,16)$    </td><td>      $46.1$       </td><td>$2.56$ GB/s</td></tr>
                          <tr><td>$(16,32)$    </td><td>      $46.7$       </td><td>$2.55$ GB/s</td></tr>
                          <tr><td>$(16,16)$    </td><td>      $47.5$       </td><td>$2.55$ GB/s</td></tr></table></p>
                <p>On my GPU, all throughput measurements are very similar!</p>
                </section>

                <section><h4>Practical example: profiling <code class="language-c">sumMatrixOnGPU2D</code></h4>
                <p>We can check memory load <i>efficiency</i> with <code class="language-c">nvprof --metrics gld_efficiency</code> (ratio of global load throughput requested to required).</p>
                <p><table><tr><td>Configuration</td><td>Execution time (ms)</td><td>Global load efficiency</td></tr>
                          <tr><td>$(32,32)$    </td><td>      $51.8$       </td><td>$100\%$</td></tr>
                          <tr><td>$(32,16)$    </td><td>      $46.1$       </td><td>$100\%$</td></tr>
                          <tr><td>$(16,32)$    </td><td>      $46.7$       </td><td>$100\%$</td></tr>
                          <tr><td>$(16,16)$    </td><td>      $47.5$       </td><td>$100\%$</td></tr></table></p>
                </section>

                <section><h4>Practical example: profiling <code class="language-c">sumMatrixOnGPU2D</code></h4>
                <p>We can try other configurations, $(64,2)$, $(64,4)$, $(64,8)$, $(128,2)$, $(128,4)$ etc. $(256, 8)$ gives an error... why?</p>
                <p><img src="execution_model_figs/metrics.png"></p>
                </section>

                <section><h3>Parallel Reduction</h3>
                <p>The paradigmatic example of a reduction operation is to calculate the sum of an array of numbers. Sequentially this is easy to implement:</p>
                <p><pre><code class="language-c">int sum = 0;
for (int i = 0; i &lt N; i++)
    sum += array[i];</code></pre></p>
                </section>

                <section><h4>Parallel Reduction</h4>
                <p>Addition is an associative and commutative operation, thus we can sum the elements in any order. This enables parallelisation:</p>
                <p><ol><li>Partition the vector into chunks.</li>
                       <li>Each thread calculates the partial sum for its chunk.</li>
                       <li>Add partial results to get final sum.</li></ol></p>
                </section>

                <section><h4>Parallel Reduction</h4>
                <p>One common solution is using <i>iterative pairwise summation</i>:</p>
                <p><ul><li>Each chunk is a pair of elements, which are summed with the result stored in-place in the original vector.</li>
                       <li>New values are then used as input in the next iteration.</li>
                       <li>The number of input values halves on every iteration.</li></ul></p>
                <p>We can use <b>neighbour pairs</b> or <b>interleaved pairs</b> (pairs are separated in the array).</p>
                </section>

                <section><h4>Parallel Reduction</h4>
                <p>Neighbour pairs</p>
                <img src="execution_model_figs/figure_3_19.png">
                <p>For $N$ elements, this requires $N-1$ sums and $\log_2 N$ steps.</p>
                </section>

                <section><h4>Parallel Reduction</h4>
                <p>Interleaved pairs</p>
                <img src="execution_model_figs/figure_3_20.png">
                <p>The inputs to a thread are separated (<i>strided</i>) by half the length of the input on each step.</p>
                </section>

                <section><h4>A (recursive) serial implementation</h4>
                <p>Recursive (non-parallel) implementation of interleaved method:</p>
                <p><pre><code class="language-c">int recursiveReduce(int *data, int const size) {
    // terminate check
    if (size == 1) return data[0];

    // renew the stride
    int const stride = size / 2;

    // in-place reduction
    for (int i = 0; i &lt stride; i++) {
        data[i] += data[i + stride];
    }

    // call recursively
    return recursiveReduce(data, stride);
}</code></pre></p>
                </section>

                <section><h4>Parallel reduction</h4>
                <img src="execution_model_figs/figure_3_21.png">
                </section>

                <section><h4>Parallel reduction: kernel code</h4>
                <p><pre><code class="language-c">__global__ void reduceNeighbored(int *g_idata, int *g_odata, unsigned int n) {
    // set thread ID
    unsigned int tid = threadIdx.x;

    // convert global data pointer to the local pointer of this block
    int *idata = g_idata + blockIdx.x * blockDim.x;

    // boundary check
    if (tid >= n) return;

    // in-place reduction in global memory
    for (int stride = 1; stride &lt blockDim.x; stride *= 2) {
        if ((tid % (2 * stride)) == 0) {
            idata[tid] += idata[tid + stride];
        }

        // synchronize within block
        __syncthreads();
    }

    // write result for this block to global mem
    if (tid == 0) g_odata[blockIdx.x] = idata[0];
}</code></pre></p>
                </section>

                <section><h4>Parallel reduction: method</h4>
                <p><ul><li>Each thread block operates independently on a part of the full array $A$.</li>
                       <li>Kernel has 2 global memory arrays: one for $A$, another smaller array for the partial sums in each thread block.</li>
                       <li>Each iteration of a loop performs one step of the reduction.</li>
                       <li>Reduction is done <i>in-place</i> (values in global memory $A$ are replaced at each step).</li>
                       <li><code class="language-c">__syncthreads</code> is used to ensure that all partial sums in current iteration have been saved to global memory before any threads in the same thread block begin the next iteration.</li>
                       <li>After final iteration, the sum for the entire thread block is saved to global memory.</li></ul></p>
                </section>

                <section><h4>Parallel reduction: method</h4>
                <p><ul><li>The distance (<i>stride</i>) between two neighbouring elements starts at $1$.</li>
                       <li>In each iteration this is doubled.</li>
                       <li>After first iteration the even elements of <code class="language-c">idata</code> are replaced with partial sums.</li>
                       <li>After second iteration every fourth element of <code class="language-c">idata</code> is replaced with a partial sum.</li>
                       <li>As there is <b>no synchronisation between thread blocks</b> the results for each block are copied back to the host and sequentially summed on the CPU.</li></ul></p>
                </section>

                <section><h4>Parallel reduction: method</h4>
                <img src="execution_model_figs/figure_3_22.png">
                </section>

                <section><h4>Parallel reduction: main function</h4>
                <p><pre><code class="language-c">int main(int argc, char **argv) {
    // set up device
    int dev = 0;
    cudaDeviceProp deviceProp;
    cudaGetDeviceProperties(&deviceProp, dev);
    printf("%s starting reduction at ", argv[0]);
    printf("device %d: %s ", dev, deviceProp.name);
    cudaSetDevice(dev);

    bool bResult = false;

    // initialization
    int size = 1&lt&lt24; // total number of elements to reduce
    printf("    with array size %d ", size);

    // execution configuration
    int blocksize = 512;    // initial block size
    if(argc &gt 1) {
        blocksize = atoi(argv[1]);    // block size from command line argument
    }
    dim3 block (blocksize,1);
    dim3 grid ((size+block.x-1)/block.x,1);
    printf("grid %d block %d\n",grid.x, block.x);

    // allocate host memory
    size_t bytes = size * sizeof(int);
    int *h_idata = (int *) malloc(bytes);
    int *h_odata = (int *) malloc(grid.x*sizeof(int));
    int *tmp = (int *) malloc(bytes);

    // initialize the array
    for (int i = 0; i &lt size; i++) {
        // mask off high 2 bytes to force max number to 255
        h_idata[i] = (int)(rand() & 0xFF);
    }
    memcpy (tmp, h_idata, bytes);

    size_t iStart,iElaps;
    int gpu_sum = 0;

    // allocate device memory
    int *d_idata = NULL;
    int *d_odata = NULL;
    cudaMalloc((void **) &d_idata, bytes);
    cudaMalloc((void **) &d_odata, grid.x*sizeof(int));

    // cpu reduction
    iStart = seconds ();
    int cpu_sum = recursiveReduce(tmp, size);
    iElaps = seconds () - iStart;
    printf("cpu reduce elapsed %d ms cpu_sum: %d\n",iElaps,cpu_sum);

    // kernel 1: reduceNeighbored
    cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice);
    cudaDeviceSynchronize();
    iStart = seconds ();
    warmup&lt&lt&ltgrid, block&gt&gt&gt(d_idata, d_odata, size);
    cudaDeviceSynchronize();
    iElaps = seconds () - iStart;
    cudaMemcpy(h_odata, d_odata, grid.x*sizeof(int), cudaMemcpyDeviceToHost);
    gpu_sum = 0;
    for (int i=0; i&ltgrid.x; i++) gpu_sum += h_odata[i];
    printf("gpu Warmup    elapsed %d ms gpu_sum: %d &lt&lt&ltgrid %d block %d&gt&gt&gt\n",
        iElaps,gpu_sum,grid.x,block.x);

    // kernel 1: reduceNeighbored
    cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice);
    cudaDeviceSynchronize();
    iStart = seconds ();
    reduceNeighbored&lt&lt&ltgrid, block&gt&gt&gt(d_idata, d_odata, size);
    cudaDeviceSynchronize();
    iElaps = seconds () - iStart;
    cudaMemcpy(h_odata, d_odata, grid.x*sizeof(int), cudaMemcpyDeviceToHost);
    gpu_sum = 0;
    for (int i=0; i&ltgrid.x; i++) gpu_sum += h_odata[i];
    printf("gpu Neighbored elapsed %d ms gpu_sum: %d &lt&lt&ltgrid %d block %d&gt&gt&gt\n",
        iElaps,gpu_sum,grid.x,block.x);

    /// free host memory
    free(h_idata);
    free(h_odata);

    // free device memory
    cudaFree(d_idata);
    cudaFree(d_odata);

    // reset device
    cudaDeviceReset();

    // check the results
    bResult = (gpu_sum == cpu_sum);
    if(!bResult) printf("Test failed!\n");
    return EXIT_SUCCESS;
}</code></pre></p>
                </section>

                <section><h4>Parallel reduction: divergence</h4>
                <p>The conditional in the kernel causes divergent warps:</p>
                <p><code class="language-c">if ((tid % (2 * stride)) == 0)</code></p>
                <p>Only even-numbered threads do any work on each iteration.</p>
                <p>Warp divergence can be reduced by arranging the thread indices such that neighbouring threads perform the addition.</p>
                </section>

                <section><h4>Parallel reduction: reduced divergence</h4>
                <p><img src="execution_model_figs/figure_3_23.png"></p>
                </section>

                <section><h4>Parallel reduction: reduced divergence, kernel code</h4>
                <p><pre><code class="language-c">__global__ void reduceNeighboredLess (int *g_idata, int *g_odata, unsigned int n) {
    // set thread ID
    unsigned int tid = threadIdx.x;
    unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;

    // convert global data pointer to the local pointer of this block
    int *idata = g_idata + blockIdx.x*blockDim.x;

    // boundary check
    if(idx &gt= n) return;

    // in-place reduction in global memory
    for (int stride = 1; stride &lt blockDim.x; stride *= 2) {
        // convert tid into local array index
        int index = 2 * stride * tid;
        if (index &lt blockDim.x) {
            idata[index] += idata[index + stride];
        }

        // synchronize within threadblock
        __syncthreads();
    }

    // write result for this block to global mem
    if (tid == 0) g_odata[blockIdx.x] = idata[0];
}</code></pre></p>
                </section>

                <section><h4>Parallel reduction: reduced divergence</h4>
                <p><ul><li>Array index is set with <code class="language-c">int index = 2 * stride * tid;</code> and conditional is removed.</li>
                       <li>Using <code class="language-c">if (index &lt blockDim.x)</code> ensures that the first half of a thread block executes the addition (because the stride is multiplied by 2)</li>
                       <li>With $512$ threads in a block, the first $8$ warps execute on the first iteration, then the first $4$ etc.</li>
                       <li>Divergence occurs in later iterations when the total number of threads working is less than the warp size.</li></ul></p>
                </section>

                <section><h4>Parallel reduction: modified main</h4>
                <p>The call to the new kernel is added to the main function:</p>
                <p><pre><code class="language-c">// kernel 2: reduceNeighbored with less divergence
cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice);
cudaDeviceSynchronize();
iStart = seconds();
reduceNeighboredLess&lt&lt&ltgrid, block&gt&gt&gt(d_idata, d_odata, size);
cudaDeviceSynchronize();
iElaps = seconds() - iStart;
cudaMemcpy(h_odata, d_odata, grid.x*sizeof(int), cudaMemcpyDeviceToHost);
gpu_sum = 0;
for (int i=0; i &lt grid.x; i++) gpu_sum += h_odata[i];
printf("gpu Neighbored2 elapsed %d ms gpu_sum: %d &lt&lt&ltgrid %d block %d&gt&gt&gt\n",
    iElaps,gpu_sum,grid.x,block.x);</code></pre></p>
                </section>

                <section><h4>Parallel reduction: reduced divergence</h4>
                <p><ul><li>The new kernel is about twice as fast as the old one (on my laptop). Why? We use the profiler.</li>
                       <li>The metric <code class="language-c">inst_per_warp</code> tells us the average number of instructions executed by each warp (more instructions per warp indicates high divergence).</li>
                       <li>With <code class="language-c">gld_throughput</code> we can see memory load throughput (higher value indicates faster data transfer).</li></ul></p>
                <p><table><tr><td>Code</td><td>Inst. per warp</td><td>Load throughput</td></tr>
                          <tr><td>Kernel 1</td><td>$417.9$</td><td>$29.2$ GB/s</td></tr>
                          <tr><td>Kernel 2</td><td>$149.6$</td><td>$55.2$ GB/s</td></tr></table></p>
                </section>

                <section><h4>Parallel reduction: interleaved pairs</h4>
                <p>We can reverse the striding by starting with a stride of half the block size and then reducing by half on each iteration:</p>
                <p><img src="execution_model_figs/figure_3_24.png" height=300></p>
                <p>This changes the load/store locations in global memory for each thread. We will see later why this is important.</p>
                </section>

                <section><h4>Parallel reduction: interleaved pairs, kernel code</h4>
                <p><pre><code class="language-c">__global__ void reduceInterleaved (int *g_idata, int *g_odata, unsigned int n) {
    // set thread ID
    unsigned int tid = threadIdx.x;
    unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;

    // convert global data pointer to the local pointer of this block
    int *idata = g_idata + blockIdx.x * blockDim.x;

    // boundary check
    if(idx &gt= n) return;

    // in-place reduction in global memory
    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {
        if (tid &lt stride) {
            idata[tid] += idata[tid + stride];
        }

        __syncthreads();
    }

    // write result for this block to global mem
    if (tid == 0) g_odata[blockIdx.x] = idata[0];
}</code></pre></p>
                </section>

                <section><h4>Parallel reduction: interleaved pairs</h4>
                <p><ul><li>The stride between elements starts at half a block size, then is reduced by half in each iteration:
                       <code class="language-c">for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {</code></li>
                       <li>Then the first half executes the addition on the first iteration, the second half on the second iteration, etc. by using <code class="language-c">if (tid &lt stride)</code></li></ul></p>
                </section>

                <section><h4>Parallel reduction: interleaved pairs, main function</h4>
                <p><pre><code class="language-c">cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice);
cudaDeviceSynchronize();
iStart = seconds();
reduceInterleaved &lt&lt&lt grid, block &gt&gt&gt (d_idata, d_odata, size);
cudaDeviceSynchronize();
iElaps = seconds() - iStart;
cudaMemcpy(h_odata, d_odata, grid.x*sizeof(int), cudaMemcpyDeviceToHost);
gpu_sum = 0;
for (int i = 0; i &lt grid.x; i++) gpu_sum += h_odata[i];
printf("gpu Interleaved elapsed %f sec gpu_sum: %d &lt&lt&ltgrid %d block %d&gt&gt&gt\n",
    iElaps,gpu_sum,grid.x,block.x);</code></pre></p>
                </section>

                <section><h4>Parallel reduction: interleaved pairs</h4>
                <p>On my GPU the interleaved kernel is about 1.4 times faster than the reduced divergence kernel, and about 2.3 times faster than the original kernel.</p>
                <p>The performance improvement is because of global memory load/store patterns. We will learn about this when we study GPU memory in Chapter 4.</p>
                </section>

                <section><h4>Parallel reduction: unrolling loops</h4>
                <p><ul><li>Loop unrolling attempts to optimize loop execution by reducing the frequency of brnaches and loop maintenance instructions.</li>
                       <li>This is done by explicitly writing the repeated code multiple times in the body of the loop and then reducing the loop iterations or removing it entirely.</li>
                       <li>The number of copies of the loop body is called the <i>loop unrolling factor</i>. The number of iterations is divided by the loop unrolling factor.</li></ul></p>
                </section>

                <section><h4>Loop unrolling example</h4>
                <p>Consider the following simple sequential loop:</p>
                <p><pre><code>for (int i = 0; i &lt 100; i++) {
    a[i] = b[i] + c[i];
}</code></pre></p>
                </section>

                <section><h4>Loop unrolling example</h4>
                <p>Loop unrolling factor of 2:</p>
                <p><pre><code>for (int i = 0; i &lt 100; i += 2) {
    a[i]   = b[i]   + c[i];
    a[i+1] = b[i+1] + c[i+1];
}</code></pre></p>
                </section>

                <section><h4>Loop unrolling example</h4>
                <p><ul><li>In this example the <code>i &lt 100</code> check is performed $100$ times in the original loop, $50$ times in the unrolled loop.</li>
                       <li>The read/write operations in each statement are independent, so the CPU can issue simultaneous memory instructions.</li>
                       <li>Note that most modern compilers can automatically unroll simple loops like this to maximise performance.</li>
                       <li>In CUDA loop unrolling can help to reduce instruction overhead and creating more independent instructions to schedule, increasing the number of eligible warps and hiding latency.</li></ul></p>
                </section>

                <section><h4>Parallel reduction: loop unrolling</h4>
                <p>Let's modify the <code>reduceInterleaved</code> kernel:</p>
                <p><pre><code>__global__ void reduceUnrolling2 (int *g_idata, int *g_odata, unsigned int n) {
    // set thread ID
    unsigned int tid = threadIdx.x;
    unsigned int idx = blockIdx.x * blockDim.x * 2 + threadIdx.x;

    // convert global data pointer to the local pointer of this block
    int *idata = g_idata + blockIdx.x * blockDim.x * 2;

    // unrolling 2 data blocks
    if (idx + blockDim.x &lt n) g_idata[idx] += g_idata[idx + blockDim.x];
    __syncthreads();

    // in-place reduction in global memory
    for (int stride = blockDim.x / 2; stride &gt 0; stride &gt&gt= 1) {
        if (tid &lt stride) {
            idata[tid] += idata[tid + stride];
        }

        // synchronize within threadblock
        __syncthreads();
    }

    // write result for this block to global mem
    if (tid == 0) g_odata[blockIdx.x] = idata[0];
}</code></pre></p>
                </section>

                <section><h4>Parallel reduction: loop unrolling</h4>
                <p><ul><li>The important change is this line: <code>if (idx + blockDim.x &lt n) g_idata[idx] += g_idata[idx + blockDim.x];</code></li>
                <li>Conceptually this is like an iteration of the reduction loop that applies the reduction across data blocks.</li>
                <li>The global array index is then adjusted accordingly as only half the number of thread blocks are now required:</li></ul></p>
                <p><pre><code>unsigned int idx = blockIdx.x * blockDim.x * 2 + threadIdx.x;
int *idata = g_idata + blockIdx.x * blockDim.x * 2;</code></pre></p>
                <img src="execution_model_figs/figure_3_25.png">
                </section>

                <section><h4>Parallel reduction: loop unrolling</h4>
                <p>The kernel call now requires half the number of blocks:</p>
                <p><pre><code>cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice);
cudaDeviceSynchronize();
iStart = seconds();
reduceUnrolling2 &lt&lt&lt grid.x/2, block &gt&gt&gt (d_idata, d_odata, size);
cudaDeviceSynchronize();
iElaps = seconds() - iStart;
cudaMemcpy(h_odata, d_odata, grid.x/2*sizeof(int), cudaMemcpyDeviceToHost);
gpu_sum = 0;
for (int i = 0; i &lt grid.x / 2; i++) gpu_sum += h_odata[i];
printf("gpu Unrolling2 elapsed %f sec gpu_sum: %d &lt&lt&ltgrid %d block %d&gt&gt&gt\n",
    iElaps,gpu_sum,grid.x/2,block.x);</code></pre></p>
                </section>

                <section><h4>Parallel reduction: loop unrolling</h4>
                <p>Timings for varying unroll factors:</p>
                <p><table><tr><td>Factor</td><td>Timing</td></tr>
                          <tr><td>   2  </td><td>0.004827 s</td></tr>
                          <tr><td>   4  </td><td>0.002761 s</td></tr>
                          <tr><td>   8  </td><td>0.001956 s</td></tr></table></p>
                </section>

                <section><h4>Parallel reduction: loop unrolling</h4>
                <p>These improvements are closely related to the data throughput achieved by giving more work to the active warps (checked in <code>nvprof</code> with <code>dram_read_throughout</code> metric).</p>
                <p><table><tr><td>Factor</td><td>DRAM read throughput</td></tr>
                          <tr><td>   2  </td><td>13.7 GB/s</td></tr>
                          <tr><td>   4  </td><td>23.8 GB/s</td></tr>
                          <tr><td>   8  </td><td>40.5 GB/s</td></tr></table></p>
                </section>

                <section><h4>Parallel reduction: unrolled warps</h4>
                <p>Once the number of values to be reduced is less than the size of a warp the <code>__syncthreads</code> instructions are irrelevant (there is implicit intra-warp synchronisation after each instruction).</p>
                <p>We can unroll the last $6$ iterations of the reduction loop as follows:</p>
                <p><pre><code>if (tid &lt 32) {
    volatile int *vmem = idata;
    vmem[tid] += vmem[tid + 32];
    vmem[tid] += vmem[tid + 16];
    vmem[tid] += vmem[tid + 8];
    vmem[tid] += vmem[tid + 4];
    vmem[tid] += vmem[tid + 2];
    vmem[tid] += vmem[tid + 1];
}</code></pre></p>
                </section>

                <section><h4>Parallel reduction: unrolled warps</h4>
                <p>The <code>volatile</code> qualifier tells the compiler to store <code>vmem[tid]</code> back to global memory with every assignment.</p>
                <p>Not using this qualifier means the compiler may attempt to optimise the code by only storing the variable in a cache or register, which will cause the code to not work properly.</p>
                <p><code>volatile</code> causes the compiler to assume the value can be changed or used at any time by other threads, and so it must be written to global memory.</p>
                </section>

                <section><h4>Parallel reduction: unrolled warps</h4>
                <p><pre><code>__global__ void reduceUnrollWarps8 (int *g_idata, int *g_odata, unsigned int n)
{
    // set thread ID
    unsigned int tid = threadIdx.x;
    unsigned int idx = blockIdx.x * blockDim.x * 8 + threadIdx.x;

    // convert global data pointer to the local pointer of this block
    int *idata = g_idata + blockIdx.x * blockDim.x * 8;

    // unrolling 8
    if (idx + 7 * blockDim.x &lt n)
    {
        int a1 = g_idata[idx];
        int a2 = g_idata[idx + blockDim.x];
        int a3 = g_idata[idx + 2 * blockDim.x];
        int a4 = g_idata[idx + 3 * blockDim.x];
        int b1 = g_idata[idx + 4 * blockDim.x];
        int b2 = g_idata[idx + 5 * blockDim.x];
        int b3 = g_idata[idx + 6 * blockDim.x];
        int b4 = g_idata[idx + 7 * blockDim.x];
        g_idata[idx] = a1 + a2 + a3 + a4 + b1 + b2 + b3 + b4;
    }

    __syncthreads();

    // in-place reduction in global memory
    for (int stride = blockDim.x / 2; stride &gt 32; stride &gt&gt= 1)
    {
        if (tid &lt stride)
        {
            idata[tid] += idata[tid + stride];
        }

        // synchronize within threadblock
        __syncthreads();
    }

    // unrolling warp
    if (tid &lt 32)
    {
        volatile int *vmem = idata;
        vmem[tid] += vmem[tid + 32];
        vmem[tid] += vmem[tid + 16];
        vmem[tid] += vmem[tid +  8];
        vmem[tid] += vmem[tid +  4];
        vmem[tid] += vmem[tid +  2];
        vmem[tid] += vmem[tid +  1];
    }

    // write result for this block to global mem
    if (tid == 0) g_odata[blockIdx.x] = idata[0];
}</code></pre></p>
                </section>

                <section><h4>Parallel reduction: unrolled warps</h4>
                <p>The profiler tells us that there are fewer <i>stalled warps</i> due to the use of <code>__syncthreads</code> (checked in <code>nvprof</code> with <code>stall_sync</code> metric).</p>
                <p><table><tr><td>Kernel</td><td>Stall percentage</td></tr>
                          <tr><td>8x unroll</td><td>49.9%</td></tr>
                          <tr><td>8x unroll, warp unroll</td><td>35.8%</td></tr></table></p>
                </section>

                <section><h4>Parallel reduction: complete unroll</h4>
                <p><ul><li>If the number of iterations in a loop is known at compile-time, the loop can be completely unrolled.</li>
                       <li>The maximum number of threads per block is $1024$ and the loop iteration count in the reduction operation is based on thread block dimension.</li>
                       <li>The reduction operation can therefore be completely unrolled.</li></ul></p>
                </section>

                <section><h4>Parallel reduction: complete unroll</h4>
                <p><pre><code>__global__ void reduceCompleteUnrollWarps8 (int *g_idata, int *g_odata,
        unsigned int n)
{
    // set thread ID
    unsigned int tid = threadIdx.x;
    unsigned int idx = blockIdx.x * blockDim.x * 8 + threadIdx.x;

    // convert global data pointer to the local pointer of this block
    int *idata = g_idata + blockIdx.x * blockDim.x * 8;

    // unrolling 8
    if (idx + 7 * blockDim.x &lt n)
    {
        int a1 = g_idata[idx];
        int a2 = g_idata[idx + blockDim.x];
        int a3 = g_idata[idx + 2 * blockDim.x];
        int a4 = g_idata[idx + 3 * blockDim.x];
        int b1 = g_idata[idx + 4 * blockDim.x];
        int b2 = g_idata[idx + 5 * blockDim.x];
        int b3 = g_idata[idx + 6 * blockDim.x];
        int b4 = g_idata[idx + 7 * blockDim.x];
        g_idata[idx] = a1 + a2 + a3 + a4 + b1 + b2 + b3 + b4;
    }

    __syncthreads();

    // in-place reduction and complete unroll
    if (blockDim.x &gt= 1024 && tid &lt 512) idata[tid] += idata[tid + 512];

    __syncthreads();

    if (blockDim.x &gt= 512 && tid &lt 256) idata[tid] += idata[tid + 256];

    __syncthreads();

    if (blockDim.x &gt= 256 && tid &lt 128) idata[tid] += idata[tid + 128];

    __syncthreads();

    if (blockDim.x &gt= 128 && tid &lt 64) idata[tid] += idata[tid + 64];

    __syncthreads();

    // unrolling warp
    if (tid &lt 32)
    {
        volatile int *vsmem = idata;
        vsmem[tid] += vsmem[tid + 32];
        vsmem[tid] += vsmem[tid + 16];
        vsmem[tid] += vsmem[tid +  8];
        vsmem[tid] += vsmem[tid +  4];
        vsmem[tid] += vsmem[tid +  2];
        vsmem[tid] += vsmem[tid +  1];
    }

    // write result for this block to global mem
    if (tid == 0) g_odata[blockIdx.x] = idata[0];
}</code></pre></p>
                </section>

                <section><h4>Parallel reduction: optimisations</h4>
                <p>Timings and speedup:</p>
                <p><table><tr><td>Kernel                   </td><td>Time (s)</td><td>Step speedup</td><td>Cumulative speedup</td></tr>
                          <tr><td>Neighbour (divergent)    </td><td>0.0182</td><td>--   </td><td>--  </td></tr>
                          <tr><td>Neighbour (non divergent)</td><td>0.0106</td><td>1.72 </td><td> 1.72 </td></tr>
                          <tr><td>Interleaved              </td><td>0.0084</td><td>1.26 </td><td> 2.17 </td></tr>
                          <tr><td>Unrolled x2              </td><td>0.0048</td><td>1.75 </td><td> 3.80 </td></tr>
                          <tr><td>Unrolled x4              </td><td>0.0033</td><td>1.45 </td><td> 5.51 </td></tr>
                          <tr><td>Unrolled x8              </td><td>0.0020</td><td>1.65 </td><td> 9.09 </td></tr>
                          <tr><td>Unrolled x8 and warps    </td><td>0.0014</td><td>1.43 </td><td>13.00  </td></tr>
                          <tr><td>Complete unroll          </td><td>0.0017</td><td>0.82!!</td><td>10.66!!</td></tr></table></p>
                </section>

                <section><h4>Parallel reduction: optimisations</h4>
                <p>The fastest optimised kernel that we have (interleaved threads, 8x unrolling across blocks and warp unrolling) has about 2.3 times more lines of code than the simplest kernel we started with...</p>
                <p class="fragment">...but it runs $13$ times faster!</p>
                </section>

                <section><h4>Parallel reduction: optimisations</h4>
                <p>Comparison of global memory load/store efficiency:</p>
                <p><table><tr><td>Kernel                  </td><td>Time (s)</td><td>Load efficiency</td><td>Store efficiency</td></tr>
                          <tr><td>Neighbour (divergent)    </td><td>0.0182  </td><td>25.0% </td><td> 25.0%  </td></tr>
                          <tr><td>Neighbour (non divergent)</td><td>0.0106  </td><td>25.0% </td><td> 25.0% </td></tr>
                          <tr><td>Interleaved              </td><td>0.0084  </td><td>96.2% </td><td> 95.5% </td></tr>
                          <tr><td>Unrolled x2              </td><td>0.0048  </td><td>98.0% </td><td> 97.7% </td></tr>
                          <tr><td>Unrolled x4              </td><td>0.0033  </td><td>98.7% </td><td> 97.7% </td></tr>
                          <tr><td>Unrolled x8              </td><td>0.0020  </td><td>99.2% </td><td> 97.7% </td></tr>
                          <tr><td>Unrolled x8 and warps    </td><td>0.0014  </td><td>99.4% </td><td> 99.4% </td></tr>
                          <tr><td>Complete unroll          </td><td>0.0017  </td><td>99.4% </td><td> 99.4%</td></tr></table></p>
                </section>

                <section><h3>Dynamic parallelism</h3>
                <p><ul><li>Parent grid may launch a <i>child</i> grid on the GPU.</li>
                       <li>Execution of a thread block is not complete until all child grids created by all threads in the block have completed.</li>
                       <li>If no explicit synchronisation between parent/child is set then an implicit synchronisation is triggered when the block attempts to exit.</li>
                       <li>Child grid is not guaranteed to begin execution until parent thread block explicitly synchronises with the child.</li>
                       <li>Parent and child grids share same global and constant memory, but have different local and shared memory.</li>
                       <li>All global memory operations in the the parent prior to invocation of the child are visible to the child grid. The operations of the child grid are only visible to the parent after synchronisation when the child completes.</li></ul></p>
                </section>

                <section><h4>Dynamic parallelism</h4>
                <p><img src="execution_model_figs/figure_3_26.png"></p>
                </section>

                <section><h4>Dynamic parallelism: nested hello world</h4>
                <style>
           .reveal pre code {
                      max-height: 300px;
            }</style>
                <p><pre><code>__global__ void nestedHelloWorld(int const iSize, int iDepth)
{
    int tid = threadIdx.x;
    printf("Recursion=%d: Hello World from thread %d block %d\n", iDepth, tid,
           blockIdx.x);

    // condition to stop recursive execution
    if (iSize == 1) return;

    // reduce block size to half
    int nthreads = iSize >> 1;

    // thread 0 launches child grid recursively
    if(tid == 0 && nthreads > 0)
    {
        nestedHelloWorld&lt&lt&lt1, nthreads&gt&gt&gt(nthreads, ++iDepth);
        printf("-------&gt nested execution depth: %d\n", iDepth);
    }
}</code></pre></p>
                </section>

                <section><h4>Dynamic parallelism: nested hello world</h4>
                <p>To compile:</p>
                <p><code>nvcc -arch=sm_50 nestedHelloWorld.cu -o nestedHelloWorld.x -lcudadevrt -rdc=true</code></p>
                <p><code>-lcudadevrt</code> explicitly links the device runtime library and <code>-rdc=true</code> generates relocatable device code (both required for dynamic parallelism).</p>
                <p>We can visually check the kernel execution using:</p>
                <p><code>nvprof --export-profile nestHelloWorld.nvvp ./nestedHelloWorld.x</code></p>
                </section>

                <section><h4>Dynamic parallelism: nested hello world</h4>
                <p><img src="execution_model_figs/figure_3_27.png"></p>
                </section>

                <section><h4>Dynamic parallelism: nested hello world</h4>
                <p>We can invoke 2 blocks in the parent grid using: <code>./nestedHelloWorld.x 2</code></p>
                <p><img src="execution_model_figs/figure_3_29.png"></p>
                </section>

                <section><h4>Dynamic parallelism: nested hello world</h4>
                <p><img src="execution_model_figs/restrictions_dynamic.png"></p>
                </section>

                <section><h4>Dynamic parallelism: recursive reduce</h4>
                <p><pre><code>__global__ void gpuRecursiveReduce (int *g_idata, int *g_odata, unsigned int isize)
{
    // set thread ID
    unsigned int tid = threadIdx.x;

    // convert global data pointer to the local pointer of this block
    int *idata = g_idata + blockIdx.x * blockDim.x;
    int *odata = &g_odata[blockIdx.x];

    // stop condition
    if (isize == 2 && tid == 0)
    {
        g_odata[blockIdx.x] = idata[0] + idata[1];
        return;
    }

    // nested invocation
    int istride = isize &gt&gt 1;

    if(istride &gt 1 && tid &lt istride)
    {
        // in place reduction
        idata[tid] += idata[tid + istride];
    }

    // sync at block level
    __syncthreads();

    // nested invocation to generate child grids
    if(tid == 0)
    {
        gpuRecursiveReduce&lt&lt&lt1, istride&gt&gt&gt(idata, odata, istride);

        // sync all child grids launched in this block
        cudaDeviceSynchronize();
    }

    // sync at block level again
    __syncthreads();
}</code></pre></p>
                </section>

                <section><h4>Dynamic parallelism: recursive reduce</h4>
                <p><ul><li>It's very slow!</li>
                       <li>There are $2048$ blocks initially.</li>
                       <li>Each performs $8$ recursions, so there are $16384$ child blocks.</li>
                       <li>Thus there are $16384$ invocations of <code>__syncthreads</code>.</li>
                       <li>We can remove the synchronisation for the child blocks because they can use their parent's values without worrying about synchronisation.</li></ul></p>
                </section>

                <section><h4>Dynamic parallelism: recursive reduce (no sync)</h4>
                <p><pre><code>__global__ void gpuRecursiveReduceNosync (int *g_idata, int *g_odata, unsigned int isize)
{
    // set thread ID
    unsigned int tid = threadIdx.x;

    // convert global data pointer to the local pointer of this block
    int *idata = g_idata + blockIdx.x * blockDim.x;
    int *odata = &g_odata[blockIdx.x];

    // stop condition
    if (isize == 2 && tid == 0)
    {
        g_odata[blockIdx.x] = idata[0] + idata[1];
        return;
    }

    // nested invoke
    int istride = isize &gt&gt 1;

    if(istride &gt 1 && tid &lt istride)
    {
        idata[tid] += idata[tid + istride];

        if(tid == 0)
        {
            gpuRecursiveReduceNosync&lt&lt&lt1, istride&gt&gt&gt(idata, odata, istride);
        }
    }
}</code></pre></p>
                </section>

                <section><h4>Dynamic parallelism: recursive reduce</h4>
                <p><ul><li>Each block launches a child grid, leading to many kernel invocations.</li>
                       <li>We can use the approach shown in the figure to increase the number of thread blocks per child grid and decrease the number of child grids, maintaining the same amount of parallelism.</li></ul></p>
                <p><img src="execution_model_figs/figure_3_30.png"></p>
                </section>

                <section><h4>Dynamic parallelism: recursive reduce (no sync, version 2)</h4>
                <p><pre><code>__global__ void gpuRecursiveReduce2(int *g_idata, int *g_odata, int iStride, int const iDim)
{
    // convert global data pointer to the local pointer of this block
    int *idata = g_idata + blockIdx.x * iDim;

    // stop condition
    if (iStride == 1 && threadIdx.x == 0)
    {
        g_odata[blockIdx.x] = idata[0] + idata[1];
        return;
    }

    // in place reduction
    idata[threadIdx.x] += idata[threadIdx.x + iStride];

    // nested invocation to generate child grids
    if(threadIdx.x == 0 && blockIdx.x == 0)
    {
        gpuRecursiveReduce2&lt&lt&ltgridDim.x, iStride / 2&gt&gt&gt(g_idata, g_odata,
                iStride / 2, iDim);
    }
}
</code></pre></p>
                </section>


				<!-- <section><h4>GPU thread vs CPU thread</h4>
                <div class="container">
                <div class="col">
                <ul><li class="fragment" data-fragment-index=1>Threads on CPU are heavyweights - OS must swap threads on and off CPU execution channels for multithreading. This <i>context switching</i> is expensive.</li>
                    <li class="fragment" data-fragment-index=3>CPU cores minimise <i>latency</i> for one or two threads.</li>
                    <li class="fragment" data-fragment-index=5>A CPU with 4 quad-core processors can run 16 threads concurrently (32 if hyper-threading is supported).</li></ul>
                </div>
                <div class="col">
                <ul><li class="fragment" data-fragment-index=2>Threads on GPU are lightweights - 1000s of threads available, context switching is fast.</li>
                    <li class="fragment" data-fragment-index=4>GPU cores handle many concurrent lightweight threads to maximise <i>throughput</i>.</li>
                    <li class="fragment" data-fragment-index=6>Modern GPUs support 1536 active threads per mutliprocessor. On a GPU with 16 multiprocessors this means over 24000 concurrently active threads.</li></ul>
                </div>
                </div>
                </section> -->

			</div>
		</div>

	<script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.9.0/js/reveal.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.5.0/plugin/math/math.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.9.0/plugin/highlight/highlight.js"></script>
    <script>
      Reveal.initialize({
        hash: true,
        math: {
        mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
        config: 'TeX-AMS_HTML-full',
        // pass other options into `MathJax.Hub.Config()`
        TeX: { Macros: { RR: "{\\bf R}" } }
        },
        plugins: [ RevealHighlight ]
      });
    </script>

	</body>
</html>
