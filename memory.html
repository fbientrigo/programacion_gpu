
<!DOCTYPE html>
<html>
<head>

<meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="chrome=1" />

<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />


<title>Memory</title>

<!-- General and theme style sheets -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.9.0/css/reveal.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.9.0/css/theme/white.css" id="theme">
<!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.9.0/lib/css/zenburn.css"> -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.2.0/styles/sunburst.min.css">


        <style type="text/css">
            .container{
                        display: flex;
                      }
            .col{
                      flex: 1;
                }
            .reveal section p {
                      display: inline-block;
                      font-size: 0.6em;
                      line-height: 1.2em;
                      vertical-align: top;
                      text-align: left;
            }
            .reveal section li {
                      font-size: 0.6em;
            }
            .reveal section td {
                      font-size: 0.6em;
            }
            .reveal section img {
                      border: none;
            }
        </style>

	</head>
	<body>
		<div class="reveal">
			<div class="slides">

                <section><h2>GPU Memory</h2>
                </section>

				<section><h4>Memory hierarchy (host)</h4>
                <p><img src="memory_figs/figure_4_1.png"></p>
                <p>Caches and registers are <i>non-programmable</i>.</p>
                </section>

                <section><h4>Memory hierarchy (GPU)</h4>
                <p><img src="memory_figs/figure_4_2.png"></p>
                <p>All non-cache memory spaces are programmable.</p>
                </section>

                <section><h4>Registers</h4>
                <p><ul><li>Fastest type of memory. Most local variables (declared within a kernel) are stored in registers.</li>
                       <li>Register variables are private to each thread.</li>
                       <li>Registers are partitioned among active warps in an SM. Fermi GPUs have max. 63 registers per thread, Kepler 255.</li></ul></p>
                </section>

                <section><h4>Registers</h4>
                <p>Hardware resources used by a kernel can be checked with <code>nvcc</code> options.</p>
                <p>This command prints the number of registers, bytes of shared memory and bytes of constant memory used by each thread:</p>
                <p><code>-Xptas="-v"</code></p>
                <p>This passes the option <code>-v</code> to the PTXAS assembler.</p>
                <p>(Compilation process: CUDA $\xrightarrow{\text{nvcc}}$ PTX $\rightarrow$ binary code for the GPU)</p> 
                </section>

                <section><h4>Registers</h4>
                <p><ul><li>If a kernel uses more registers than the hardware limit, local memory will be used instead.</li>
                       <li>This is called <i>register spilling</i>.</li>
                       <li>The compiler will try to avoid register spilling, but this can be assisted using the following option in the kernel definition:</li></ul></p>
                <p><pre><code>__global__ void
__launch_bounds__(maxThreadsPerBlock, minBlocksPerMultiprocessor)
kernel(...) {
  // kernel body
}</code></pre></p>
                <p>The maximum number of registers to be used by all kernels can be set at compile time using <code>-maxrregcount=32</code>.</p>
                <p>This option is ignored by kernels that have <code>__launch_bounds__</code> specified.</p>
                </section>

                <section><h4>Local memory</h4>
                <p><ul><li>Kernel variables that cannot fit in registers are moved to local memory.</li>
                       <li>"Local memory" is physically just global memory (high latency, low bandwidth).</li>
                       <li>For compute capability &gt= 2.0, local memory data is cached on L1 (per SM) and L2 (per device).</li></ul></p>
                </section>

                <section><h4>Shared memory</h4>
                <p><ul><li>Variables declared in the kernel with <code>__shared__</code> are stored in shared memory.</li>
                       <li>This memory is on-chip: high bandwidth, low latency.</li>
                       <li>Each SM has a limited amount of shared memory, partitioned among thread blocks. If too much shared memory is used the number of active warps will be limited.</li>
                       <li>Shared memory allows for inter-thread communication (within a block).</li></ul></p>
                </section>

                <section><h4>Constant memory</h4>
                <p><ul><li>Resides in device memory and is cached in a dedicated, per-SM constant cache.</li>
                       <li>A constant variable is declared with <code>__constant__</code>.</li>
                       <li>These must have <i>global scope</i>, outside of any kernels. 64KB is available.</li>
                       <li>It is useful for mathematical constants that are applied by all threads.</li>
                       <li>Kernels may only read from constant memory, so it must be initialised on the host:</li></ul></p>
                <p><code>cudaError_t cudaMemcpyToSymbol(const void* symbol, const void* src, size_t count);</code></p>
                </section>

                <section><h4>Texture memory</h4>
                <p><ul><li>Resides in device memory and is cached in a per-SM, read-only cache.</li>
                       <li>The data in texture memory can be interpolated as it is read, and is optimised for 2D spatial locality.</li>
                       <li>This is useful in graphics applications where textures (surfaces) must be applied to objects.</li>
                       <li>There are sometimes useful scientific applications of texture memory, but it is rare, so we will not discuss it further.</li></ul></p>
                </section>

                <section><h4>Global memory</h4>
                <p><ul><li>Main memory of the GPU, high latency, low bandwidth.</li>
                       <li>Global memory can be declared statically (without use of allocation functions) in device code with <code>__device__</code>.</li>
                       <li>Dynamic global memory allocation uses <code>cudaMalloc</code>.</li>
                       <li>Efficient use of global memory is extremely important to optimise GPU code.</li></ul></p>
                </section>

                <section><h4>Variables and memory in CUDA</h4>
                <p><table><tr><td>Qualifier</td><td>Variable</td><td>Memory</td><td>Scope</td><td>Lifespan</td></tr>
                          <tr><td>                       </td>  <td><code>float var</code></td>     <td>Register</td><td>Thread</td><td>Thread</td></tr>
                          <tr><td>                       </td>  <td><code>float var[100]</code></td><td>Local</td><td>Thread</td><td>Thread</td></tr>
                          <tr><td><code>__shared__</code></td>  <td><code>float var[]</code></td>   <td>Shared</td><td>Block</td><td>Block</td></tr>
                          <tr><td><code>__device__</code></td>  <td><code>float var[]</code></td>   <td>Global</td><td>Global</td><td>Application</td></tr>
                          <tr><td><code>__constant__</code></td><td><code>float var[]</code></td>   <td>Constant</td><td>Global</td><td>Application</td></tr></table></p>
                </section>

                <section><h4>Memory types</h4>
                <p><table><tr><td>Memory</td><td>On/off chip</td><td>Cached    </td><td>Access</td><td>Scope</td><td>Lifespan</td></tr>
                          <tr><td>Register</td><td>On </td>      <td>N/A       </td><td>R/W   </td><td>1 thread</td><td>Thread</td></tr>
                          <tr><td>Local</td>   <td>Off</td>      <td>CC &gt 2.0</td><td>R/W   </td><td>1 thread</td><td>Thread</td></tr>
                          <tr><td>Shared</td>  <td>On </td>      <td>N/A       </td><td>R/W   </td><td>All threads in block</td><td>Block</td></tr>
                          <tr><td>Global</td>  <td>Off</td>      <td>CC &gt 2.0</td><td>R/W   </td><td>All threads + host</td><td>Host allocation</td></tr>
                          <tr><td>Constant</td><td>Off</td>      <td>Yes       </td><td>R     </td><td>All threads + host</td><td>Host allocation</td></tr>
                          <tr><td>Texture</td><td>Off</td>      <td>Yes       </td><td>R     </td><td>All threads + host</td><td>Host allocation</td></tr></table></p>
                </section>

                <section><h4>Static global memory</h4>
                <p><pre><code>#include &ltcuda_runtime.h&gt
#include &ltstdio.h&gt

__device__ float devData;

__global__ void checkGlobalVariable()
{
    // display the original value
    printf("Device: the value of the global variable is %f\n", devData);

    // alter the value
    devData += 2.0f;
}

int main(void)
{
    // initialize the global variable
    float value = 3.14f;
    cudaMemcpyToSymbol(devData, &value, sizeof(float));
    printf("Host:   copied %f to the global variable\n", value);

    // invoke the kernel
    checkGlobalVariable&lt&lt&lt1, 1&gt&gt&gt();

    // copy the global variable back to the host
    cudaMemcpyFromSymbol(&value, devData, sizeof(float));
    printf("Host:   the value changed by the kernel to %f\n", value);

    cudaDeviceReset();
    return EXIT_SUCCESS;
}</code></pre></p>
                <p>The host and device code live in different worlds: the host code cannot directly access a device variable, and device code cannot directly access a host variable. The CUDA API function calls must be used, as shown above.</p>
                </section>

                <section><h3>Memory management</h3></section>

                <section><h4>Memory allocation</h4>
                <p><ul><li><code>cudaError_t cudaMalloc(void **devPtr, size_t count);</code></li>
                       <li><code>cudaError_t cudaMemset(void *devPtr, int value, size_t count);</code></li>
                       <li><code>cudaError_t cudaFree(void *devPtr);</code></li></ul></p>
                <p>Allocation/deallocation are expensive, so memory should be reused when possible.</p>
                </section>

                <section><h4>Memory transfer</h4>
                <p><code>cudaError_t cudaMemcpy(void *dst, const void *src, size_t count, enum cudaMemcpyKind kind);</code></p>
                <p><ul><li><code>cudaMemcpyHostToHost</code></li>
                       <li><code>cudaMemcpyHostToDevice</code></li>
                       <li><code>cudaMemcpyDeviceToHost</code></li>
                       <li><code>cudaMemcpyDeviceToDevice</code></li></ul></p>
                </section>

                <section><h4>Example of memory transfer</h4>
                <p><pre><code>#include &ltcuda_runtime.h&gt
#include &ltstdio.h&gt

int main(int argc, char **argv) {
    // set up device
    int dev = 0;
    cudaSetDevice(dev);

    // memory size
    unsigned int isize = 1&lt&lt22;
    unsigned int nbytes = isize * sizeof(float);

    // get device information
    cudaDeviceProp deviceProp;
    cudaGetDeviceProperties(&deviceProp, dev);
    printf("%s starting at ", argv[0]);
    printf("device %d: %s memory size %d nbyte %5.2fMB\n", dev, deviceProp.name,isize,nbytes/(1024.0f*1024.0f));

    // allocate the host memory
    float *h_a = (float *)malloc(nbytes);

    // allocate the device memory
    float *d_a;
    cudaMalloc((float **)&d_a, nbytes);

    // initialize the host memory
    for(unsigned int i=0;i&ltisize;i++) h_a[i] = 0.5f;

    // transfer data from the host to the device
    cudaMemcpy(d_a, h_a, nbytes, cudaMemcpyHostToDevice);

    // transfer data from the device to the host
    cudaMemcpy(h_a, d_a, nbytes, cudaMemcpyDeviceToHost);

    // free memory
    cudaFree(d_a);
    free(h_a);

    // reset device
    cudaDeviceReset();
    return EXIT_SUCCESS;

}</code></pre></p>
                </section>

                <section><h4>Memory transfer</h4>
                <p><img src="memory_figs/figure_4_3.png"></p>
                <p>Example for a Fermi C2050 GPU</p>
                </section>

                <section><h4>Pinned memory</h4>
                <p><ul><li>Host memory is, by default, <i>pageable</i>.</li>
                       <li>This means the memory is organised in <i>pages</i> that can be moved by the operating system within the <i>virtual memory</i> space that includes disk memory.</li>
                       <li>When data is requested from RAM that has been moved to disk, a <i>page fault</i> occurs, and the data is moved from the disk back to main memory.</li>
                       <li>The GPU has no control over when the OS might move pageable memory.</li>
                       <li>Transfer from host to device involves allocation of <i>page-locked</i> or <i>pinned</i> memory on the host. Data is transferred from pageable to pinned, and then to the device.</li></ul></p>
                </section>

                <section><h4>Pinned memory</h4>
                <p><img src="memory_figs/figure_4_4.png"></p>
                </section>

                <section><h4>Allocation of pinned memory</h4>
                <p><ul><li><code>cudaError_t cudaMallocHost(void **devPtr, size_t count);</code></li>
                       <li><code>cudaError_t cudaFreeHost(void *ptr);</code></li></ul></p>
                <p>Using too much pinned memory can affect the overall system performance, because it reduces the virtual (pageable) memory space.</p>
                <p>Example code: <code>pinnedMemoryTransfer.cu</code></p>
                </section>

                <section><h4>Unified memory</h4>
                <p><ul><li>From CUDA 6.0, <i>Unified Memory</i> allows access to memory in one single address space for both CPU and GPU.</li>
                       <li>UM migrates data between host and device, automatically.</i>
                       <li>Depends on <i>Unified Virtual Addressing</i> (introduced in CUDA 4.0) which unified the memory address space.</li></ul></p>
                <p>Static declaration of a device variable in unified (sometimes called <i>managed</i>) memory: <code>__device__ __managed__ int y;</code></p>
                <p>Dynamic allocation of unified memory: <code>cudaError_t cudaMallocManaged(void **devPtr, size_t size, unsigned int flags=0);</code>. The pointer <code>devPtr</code> is valid on the device and the host.</p>
                </section>

                <section><h3>Memory access patterns</h3></section>

                <section><h4>Global memory access</h4>
                <p>Global memory access passes through L2 cache (32-byte cache line) and possibly L1 cache (128-byte cache line) depending on access type and GPU architecture.</p>
                <p><img src="memory_figs/figure_4_6.png"></p>
                </section>

                <section><h4>Aligned and coalesced access</h4>
                <p><ul><li>Global memory access should be <i>aligned</i> and <i>coalesced</i>.</li>
                       <li><b>Aligned</b>: first address of a device memory transaction is an even multiple of 32 (bytes) for L2 cache or 128 bytes for L1 cache.</li>
                       <li><b>Coalesced</b>: when all 32 threads in a warp access a contiguous chunk of memory.</li></ul></p>
                </section>

                <section><h4>Aligned and coalesced access</h4>
                <p><img src="memory_figs/figure_4_7.png"></p>
                <p>In this example, a single 128-byte memory transaction is required to read data from device memory.</p>
                </section>

                <section><h4>Misaligned and uncoalesced access</h4>
                <p><img src="memory_figs/figure_4_8.png"></p>
                <p>In this example, 3 128-byte transactions may be needed, one starting at offset 0 to get the first 2 data values, one starting at offset 256 to get the last 2, and one at offset 128 to get most of the data. Data values are also not contiguous.</p>
                </section>

                <section><h4>Memory load types</h4>
                <p><ul><li>Cached loads: passes through L1 cache, memory request is 128-byte transaction. On an L1 miss the request goes to L2. On an L2 miss we go to DRAM (<code>-Xptxas -dlcm=ca</code> enables L1 cache)</li>
                       <li>Uncached loads: L1 cache not used, on an L2 miss we go to DRAM (<code>-Xptxas -dlcm=cg</code> disables L1 cache)</li></ul></p>
                </section>

                <section><h4>Cached loads</h4>
                <p><img src="memory_figs/figure_4_9.png"></p>
                <p>Aligned, coalesced, 100% utilisation of bus, no unused data</p>
                </section>

                <section><h4>Cached loads</h4>
                <p><img src="memory_figs/figure_4_10.png"></p>
                <p>Aligned, non-coalesced, 100% utilisation of bus, no unused data (assuming each thread accesses a different value)</p>
                </section>

                <section><h4>Cached loads</h4>
                <p><img src="memory_figs/figure_4_11.png"></p>
                <p>Misaligned, two 128-byte segments loaded, 50% bus utilisation, half unused.</p>
                </section>

                <section><h4>Cached loads</h4>
                <p><img src="memory_figs/figure_4_11.png"></p>
                <p>Misaligned, two 128-byte segments loaded, 50% bus utilisation, half unused.</p>
                </section>

                <section><h4>Cached loads</h4>
                <p><img src="memory_figs/figure_4_12.png"></p>
                <p>Misaligned, one 128-byte segment loaded, but only one (4-byte) value needed, bus utilisation is $4/128 = 3.125$%.</p>
                </section>

                <section><h4>Cached loads</h4>
                <p><img src="memory_figs/figure_4_13.png"></p>
                <p>Misaligned, $N$ 128-byte segments loaded (where $0 &lt N \leq 32$), $N$ memory transactions for one load.</p>
                </section>

                <section><h4>Uncached loads ($32$-byte segments)</h4>
                <p><img src="memory_figs/figure_4_14.png"></p>
                <p>Aligned and coalesced.</p>
                </section>

                <section><h4>Uncached loads</h4>
                <p><img src="memory_figs/figure_4_15.png"></p>
                <p>Aligned, non-coalesced, but no loads are wasted.</p>
                </section>

                <section><h4>Uncached loads</h4>
                <p><img src="memory_figs/figure_4_16.png"></p>
                <p>Misaligned, coalesced, requires at most $5$ segments, so bus utilisation is at least 80% (improvement over cached load).</p>
                </section>

                <section><h4>Uncached loads</h4>
                <p><img src="memory_figs/figure_4_17.png"></p>
                <p>Misaligned, single 4 byte value requested, bus utilisation is $4/32=12.5$% (improvement over cached load).</p>
                </section>

                <section><h4>Uncached loads</h4>
                <p><img src="memory_figs/figure_4_18.png"></p>
                <p>Misaligned, uncoalesced, requires at most $N$ 32-byte segments, rather than $N$ 128-byte cache lines (improvement over cached load).</p>
                </section>

                <section><h4>Memory alignment example</h4>
                <p>We return to the vector addition example to see the effect of forcing misalignment...</p>
                <p>Profiler: <code>nvprof --metrics gld_efficiency ./readSegment.x</code></p>
                <p><pre><code>void sumArraysOnHost(float *A, float *B, float *C, const int n, int offset)
{
    for (int idx = offset, k = 0; idx &lt n; idx++, k++)
    {
        C[k] = A[idx] + B[idx];
    }
}

__global__ void readOffset(float *A, float *B, float *C, const int n,
                           int offset)
{
    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;
    unsigned int k = i + offset;

    if (k &lt n) C[i] = A[k] + B[k];
}</code></pre></p>
                </section>

                <section><h4>Memory writes</h4>
                <p><ul><li>Writes are done with 32-byte segments.</li>
                       <li>Can be 1, 2 or 4 segment transactions.</li></ul></p>
                <p>Ideal case:</p>
                <p><img src="memory_figs/figure_4_19.png"></p>
                </section>

                <section><h4>Memory writes</h4>
                <p>3 32-byte transactions</p>
                <p><img src="memory_figs/figure_4_20.png"></p>
                </section>

                <section><h4>Memory writes</h4>
                <p>One two-segment transactions</p>
                <p><img src="memory_figs/figure_4_21.png"></p>
                </section>

                <section><h4>Misaligned writes example</h4>
                <p><pre><code>__global__ void writeOffset(float *A, float *B, float *C, const int n, int offset) {
    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;
    unsigned int k = i + offset;
    if (k &lt n) C[k] = A[i] + B[i];
}</code></pre></p>
                <p><pre><code>void sumArraysOnHost(float *A, float *B, float *C, const int n, int offset) {
    for (int idx = offset, k = 0; idx &lt n; idx++, k++) {
        C[idx] = A[k] + B[k];
    }
}</code></pre></p>
                </section>

                <section><h4>Misaligned writes example</h4>
                <p>Memory write efficiency can be checked with:</p>
                <p>Profiler: <code>nvprof --metrics gst_efficiency ./writeSegment.x</code></p>
                </section>

                <section><h4>AoS vs. SoA</h4>
                <p>Array of structures:</p>
                <p><pre><code>struct innerStruct {
    float x;
    float y;
};</code></pre></p>
                <p>In code use <code>struct innerStruct myAoS[N];</code></p>
                </section>

                <section><h4>AoS vs. SoA</h4>
                <p>Structure of arrays:</p>
                <p><pre><code>struct innerArray {
    float x[N];
    float y[N];
};</code></pre></p>
                <p>In code use <code>struct innerArray mySoA;</code></p>
                </section>

                <section><h4>AoS vs. SoA</h4>
                <p><img src="memory_figs/figure_4_22.png"></p>
                <p>In most parallel programming situations (including CUDA), SoA is the preferred method.</p>
                </section>

                <section><h4>Simple math with AoS</h4>
                <p><pre><code>__global__ void testInnerStruct(innerStruct *data, innerStruct *result, const int n) {
    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i &lt n) {
        innerStruct tmp = data[i];
        tmp.x += 10.f;
        tmp.y += 20.f;
        result[i] = tmp;
    }
}</code></pre></p>
                <p>Full code in <code>simpleMathAoS.cu</code>.</p>
                </section>

                <section><h4>Simple math with SoA</h4>
                <p><pre><code>__global__ void testInnerArray(innerArray *data, innerArray *result, const int n) {
    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i&ltn) {
        float tmpx = data-&gtx[i];
        float tmpy = data-&gty[i];
        tmpx += 10.f;
        tmpy += 20.f;
        result-&gtx[i] = tmpx;
        result-&gty[i] = tmpy;
    }
}</code></pre></p>
                <p>Full code in <code>simpleMathSoA.cu</code>.</p>
                </section>

                <section><h4>Comparing memory access</h4>
                <p><code>nvprof --metrics gld_efficiency,gst_efficiency ./simpleMath___</code></p>
                <p>For AoS: <code>gld_efficiency, gst_efficiency 50.00%</code></p>
                <p>For SoA: <code>gld_efficiency, gst_efficiency 100.00%</code></p>
                </section>

                <section><h4>Unrolling techniques</h4>
                <p>Returning to the misaligned reads example, we will add some unrolling (over blocks as before):</p>
                <p><pre><code>__global__ void readOffsetUnroll4(float *A, float *B, float *C, const int n, int offset) {
    unsigned int i = blockIdx.x * blockDim.x * 4 + threadIdx.x;
    unsigned int k = i + offset;
    if (k + 3 * blockDim.x &lt n) {
        C[i]                  = A[k]
        C[i + blockDim.x]     = A[k + blockDim.x] + B[k + blockDim.x];
        C[i + 2 * blockDim.x] = A[k + 2 * blockDim.x] + B[k + 2 * blockDim.x];
        C[i + 3 * blockDim.x] = A[k + 3 * blockDim.x] + B[k + 3 * blockDim.x];
    }
}</code></pre></p>
                <p>Full code in <code>readSegmentUnroll.cu</code>.</p>
                </section>

                <section><h4>Unrolling techniques</h4>
                <p>Checking the load/store efficiency (<code>gld/gst_efficiency</code>)shows unrolling has no impact.</p>
                <p>Unrolling increases the number of concurrently in-flight memory operations.</p>
                <p>The number of load/store transactions (<code>gld/gst_transactions</code>) is significantly reduced in the unrolled kernel.</p>
                </section>

                <section><h4>Use of bandwith</h4>
                <p><img src="memory_figs/bandwidth_use.png"></p>
                </section>

                <section><h4>Maximum bandwidth</h4>
                <p><b>Theoretical bandwidth</b>: set by the hardware of the device.</p>
                <p><b>Effective bandwidth (EB)</b>: set by the kernel.</p>
                <p>$$\text{EB} = \frac{\text{(bytes read + bytes written)} \times 10^{-9}}{\text{time elapsed}}$$</p>
                </section>

                <section><h4>Studying bandwidth use: matrix transpose</h4>
                <p><img src="memory_figs/figure_4_23.png"></p>
                </section>

                <section><h4>Host code for transpose (out-of-place)</h4>
                <p><pre><code>void transposeHost(float *out, float *in, const int nx, const int ny) {
    for (int iy = 0; iy &lt ny; ++iy) {
        for (int ix = 0; ix &lt nx; ++ix) {
            out[ix*ny+iy] = in[iy*nx+ix];
        }
    }
}</code></pre></p>
                </section>

                <section><h4>Memory access for transpose</h4>
                <p><img src="memory_figs/figure_4_24.png"></p>
                <p><ul><li>Read: original matrix accessed by rows, coalesced access.</li>
                       <li>Writes: transpose accessed by columns, strided access.</li></ul></p>
                </section>

                <section><h4>Transpose on the GPU</h4>
                <p>Strided access is the worst access pattern on a GPU, but unavoidable here.</p>
                <p>We will now consider two kernel versions:</p>
                <p><ol><li>Reads by rows, stores by columns</li>
                       <li>Reads by columns, stores by rows</li></ol></p>
                </section>

                <section><h4>Transpose on the GPU, method 1</h4>
                <p><img src="memory_figs/figure_4_25.png"></p>
                </section>

                <section><h4>Transpose on the GPU, method 2</h4>
                <p><img src="memory_figs/figure_4_26.png"></p>
                </section>

                <section><h4>Transpose on the GPU</h4>
                <p>The two implementations are identical if L1 cache is <b>disabled</b>.</p>
                <p>If L1 is enabled, reads by column will be cached, meaning next read may be serviced by cache (rather than global).</p>
                <p>Writes are not cached in L1, so writing by column will not benefit from caching.</p>
                <p>On GPUs where L1 cache is not used for global memory access, there will be no difference.</p>
                </section>

                <section><h4>Transpose on the GPU: setting performance bounds</h4>
                <p>To have an idea of the maximum and minimum memory efficiency we can achieve we will use two kernels that copy matrices:</p>
                <p><ol><li>The first version loads and stores with rows (only coalesced access).</li>
                       <li>The second version loads and stroes with columns (strided access).</li></ol></p>
                </section>

                <section><h4>Transpose on the GPU: setting performance bounds</h4>
                <p><pre><code>__global__ void copyRow(float *out, float *in, const int nx, const int ny) {
    unsigned int ix = blockDim.x * blockIdx.x + threadIdx.x;
    unsigned int iy = blockDim.y * blockIdx.y + threadIdx.y;
    if (ix &lt nx && iy &lt ny) {
        out[iy*nx + ix] = in[iy*nx + ix];
    }
}</code></pre></p>
                </section>

                <section><h4>Transpose on the GPU: setting performance bounds</h4>
                <p><pre><code>__global__ void copyCol(float *out, float *in, const int nx, const int ny) {
    unsigned int ix = blockDim.x * blockIdx.x + threadIdx.x;
    unsigned int iy = blockDim.y * blockIdx.y + threadIdx.y;
    if (ix &lt nx && iy &lt ny) {
        out[ix*ny + iy] = in[ix*ny + iy];
    }
}</code></pre></p>
                </section>

                <section><h4>Transpose on the GPU: setting performance bounds</h4>
                <p>Full code in <code>transpose.cu</code>.</p>
                <p>Code is run with integer argument to select the kernel: <code>./transpose.cu &ltint&gt</code>. The value of <code>&ltint&gt</code> switches the kernel.</p>
                <p>On my laptop <code>CopyRow</code> kernel has effective bandwidth of 64.2 GB/s, <code>CopyCol</code> kernel has 14.1 GB/s. (Theoretical maximum of my GeForce GTX 960M GPU is 80.19 GB/s).</p>
                </section>

                <section><h4>Transpose on the GPU: comparing methods</h4>
                <p><table><tr><td>Kernel method</td><td>Time</td><td>Bandwidth</td><td>Ratio to peak</td></tr>
                          <tr><td>Load rows/store columns</td><td>0.001756 s</td><td>19.1 GB/s</td><td>24%</td></tr>
                          <tr><td>Load columns/store rows</td><td>0.000920 s</td><td>36.5 GB/s</td><td>46%</td></tr></table></p>
                </section>

                <section><h4>Transpose on the GPU: comparing methods</h4>
                <p>Load/store efficiency can be checked with the following nvprof metrics: <code>gld_efficiency,gst_efficiency</code>.</p>
                <p>For <code>ncu</code> the equivalent metrics are:</p>
                <p><ul><li><code>smsp__sass_average_data_bytes_per_sector_mem_global_op_ld.pct</code></li>
                       <li><code>smsp__sass_average_data_bytes_per_sector_mem_global_op_st.pct</code></li></ul></p>
                </section>

                <section><h3>Matrix addition with unified memory</h3>
                <p>We return to the matrix addition example, this time using unified memory.</p>
                <p>Full code is in <code>sumMatrixGPUManaged.cu</code>. This can be compared with the standard approach in <code>sumMatrixGPU.cu</code>.</p>
                <p>We can check time of API calls using <code>nvprof</code> or <code>nsys profile --stats true</code>.</p>
                </section>

                <section><h2>Shared memory</h2></section>

                <section><h4>Shared memory</h4>
                <p><img src="memory_figs/figure_5_1.png"></p>
                </section>

                <section><h4>Shared memory</h4>
                <p><ul><li>Shared memory can be declared statically or dynamically.</li>
                       <li>May be local to a kernel or globally in a source file.</li>
                       <li>Shared memory arrays may be 1D, 2D or 3D.</li></ul></p>
                </section>

                <section><h4>Shared memory - static declaration</h4>
                <p>This code statically declares a shared 2D float array:</p>
                <p><pre><code>__shared__ float tile[size_y][size_x];</code></pre></p>
                <p>If declared inside a kernel this will be local to that kernel, if declared outside any kernel it will be global to all kernels.</p>
                </section>

                <section><h4>Shared memory - dynamic declaration</h4>
                <p>This code dynamically declares a shared 2D float array:</p>
                <p><pre><code>extern __shared__ int tile[];</code></pre></p>
                <p>The size of this array is set when the kernel is invoked by passing a third argument in the kernel configuration:</p>
                <p><pre><code>kernel&lt&lt&ltgrid, block, isize * sizeof(int)&gt&gt&gt(...)</code></pre></p>
                <p>Only 1D arrays may be declared dynamically.</p>
                </section>

                <section><h4>Shared memory banks</h4>
                <p><ul><li>Shared memory is divided into 32 equal-sized modules called <i>banks</i> which can be accessed simultaneously.</li>
                       <li>When a warp issues a shared memory transaction request there are three possibilities:</li>
                       <ol><li>Parallel access: multiple addresses accessed across multiple banks (ideal case)</li>
                           <li>Serial access: multiple addresses accessed within the same bank (worst case, <i>bank conflict</i>)</li>
                           <li>Broadcast access: single address read in a single bank</li></ol></p>
                </section>

                <section><h4>Shared memory banks - parallel access</h4>
                <p><img src="memory_figs/figure_5_2.png"></p>
                </section>

                <section><h4>Shared memory banks - parallel access</h4>
                <p><img src="memory_figs/figure_5_3.png"></p>
                </section>

                <section><h4>Shared memory banks - broadcast access?</h4>
                <p><img src="memory_figs/figure_5_4.png"></p>
                <p>If the memory addresses differ in each bank, then there will be a bank conflict.</p>
                </section>

                <section><h4>Shared memory banks</h4>
                <p><img src="memory_figs/figure_5_5.png"></p>
                <p>This shows the mapping between word indices and the memory banks for 4-byte bank width.</p>
                </section>

                <section><h4>Shared memory banks</h4>
                <p><img src="memory_figs/figure_5_6.png"></p>
                <p>This shows the mapping between word indices and the memory banks for 8-byte bank width (for GPUs of compute capability 3.x or higher).</p>
                </section>

                <section><h4>No bank conflicts</h4>
                <p><img src="memory_figs/figure_5_7.png"></p>
                <p>All accesses are to different banks (8-byte bank width example).</p>
                </section>

                <section><h4>No bank conflicts</h4>
                <p><img src="memory_figs/figure_5_8.png"></p>
                <p>Accesses to the same bank, but these may be served by a single memory transaction.</p>
                </section>

                <section><h4>Bank conflict</h4>
                <p><img src="memory_figs/figure_5_9.png"></p>
                <p>Accesses to the same bank (2-way conflict), cannot be served by a single memory transaction.</p>
                </section>

                <section><h4>Bank conflict</h4>
                <p><img src="memory_figs/figure_5_10.png"></p>
                <p>Accesses to the same bank (3-way conflict), cannot be served by a single memory transaction.</p>
                </section>

                <section><h4>Memory padding</h4>
                <p>Bank conflicts can be avoided using memory padding. The following example shows the idea, assuming 5 memory banks.</p>
                <p><img src="memory_figs/figure_5_11.png" height=400></p>
                </section>

                <section><h4>Example: square shared memory</h4>
                <p>We will consider a $32 \times 32$ shared memory array.</p>
                <p><img src="memory_figs/figure_5_12.png" height=400></p>
                </section>

                <section><h4>Example: square shared memory</h4>
                <p>We can statically declare a 2D shared memory array using:</p>
                <p><pre><code>__shared__ int tile[N][N]</code></pre></p>
                <p>Accessing this from a 2D thread block is straightforward. We can use either of the following access patterns:</p>
                <p><pre><code>tile[threadIdx.y][threadIdx.x]
tile[threadIdx.x][threadIdx.y]</code></pre></p>
                <p>Which is likely to be faster?</p>
                </section>

                <section><h4>Example: square shared memory</h4>
                <p><ul><li>The ideal case is to have threads in the same warp accessing different memory banks.</li>
                       <li>Threads in the same warp have consecutive values of <code>threadIdx.x</code>.</li>
                       <li>Memory banks are indexed consecutively.</li>
                       <li>We would therefore expect <code>tile[threadIdx.y][threadIdx.x]</code> to exhibit better performance.</li></ul></p>
                </section>

                <section><h4>Kernel example: no bank conflicts</h4>
                <p><pre><code>__global__ void setRowReadRow(int *out) {
    // static shared memory
    __shared__ int tile[BDIMY][BDIMX];

    // mapping from thread index to global memory index
    unsigned int idx = threadIdx.y * blockDim.x + threadIdx.x;

    // shared memory store operation
    tile[threadIdx.y][threadIdx.x] = idx;

    // wait for all threads to complete
    __syncthreads();

    // shared memory load operation
    out[idx] = tile[threadIdx.y][threadIdx.x] ;
}</code></pre></p>
                <p>Three memory operations: store on shared memory, load on shared memory, store on global memory.</p>
                </section>

                <section><h4>Kernel example: bank conflicts</h4>
                <p><pre><code>__global__ void setColReadCol(int *out) {
    // static shared memory
    __shared__ int tile[BDIMX][BDIMY];

    // mapping from thread index to global memory index
    unsigned int idx = threadIdx.y * blockDim.x + threadIdx.x;

    // shared memory store operation
    tile[threadIdx.x][threadIdx.y] = idx;

    // wait for all threads to complete
    __syncthreads();

    // shared memory load operation
    out[idx] = tile[threadIdx.x][threadIdx.y];
}</code></pre></p>
                <p>Shared memory access will result in 32-way bank conflicts for 4-byte bank width, 16-way for 8-byte.</p>
                </section>

                <section><h4>Kernel example: bank conflicts</h4>
                <p>Full code in <code>checkSmemSquare.cu</code>.</p>
                <p>Metrics for profiler: <code>shared_load_transactions_per_request</code>, <code>shared_store_transactions_per_request</code>.</p>
                <p>No equivalents of these for <code>ncu</code> but <code>shared_efficiency</code> has the equivalent <code> 	smsp__sass_average_data_bytes_per_wavefront_mem_shared.pct</code>.</p>
                </section>

                <section><h4>Shared memory examples</h4>
                <p>The same code includes multiple kernels:</p>
                <p><ul><li>setRowReadRow: reads and writes shared memory with row-major order.</li>
                       <li>setColReadCol: reads and writes shared memory with column-major order.</li>
                       <li>setRowReadCol: writes with row-major, reads with column-major.</li>
                       <li>setRowReadColDyn: dynamic shared memory allocation (requires offsets as dynamic allocation must be 1D).</li>
                       <li>setRowReadColPad: static shared memory with padding.</li>
                       <li>setRowReadColDynPad: dynamic shared memory with padding (offsetting must be modified to skip padded memory spaces).</li></ul></p>
                </section>

                <section><h3>Matrix transpose with shared memory</h3>
                <p>We return to the matrix transpose example to use shared memory to avoid uncoalesced access to global memory. The global memory transpose kernel is:</p>
                <p><pre><code>__global__ void naiveGmem(float *out, float *in, const int nx, const int ny) {
    // matrix coordinate (ix,iy)
    unsigned int ix = blockIdx.x * blockDim.x + threadIdx.x;
    unsigned int iy = blockIdx.y * blockDim.y + threadIdx.y;

    // transpose with boundary test
    if (ix &lt nx && iy &lt ny) {
        out[ix*ny+iy]= in[iy*nx+ix];
    }
}</code></pre></p>
                <p>Global memory write operation is strided.</p>
                </section>

                <section><h4>Copy kernel</h4>
                <p>The maximum performance achievable would correspond to a copy kernel:</p>
                <p><pre><code>__global__ void copyGmem(float *out, float *in, const int nx, const int ny) {
    // matrix coordinate (ix,iy)
    unsigned int ix = blockIdx.x * blockDim.x + threadIdx.x;
    unsigned int iy = blockIdx.y * blockDim.y + threadIdx.y;

    // transpose with boundary test
    if (ix &lt nx && iy &lt ny) {
        out[iy*nx+ix]= in[iy*nx+ix];
    }
}</code></pre></p>
                <p>All memory access is coalesced in this case.</p>
                </section>

                <section><h4>Matrix transpose (global memory)</h4>
                <p>Full example code in <code>transposeRectangle.cu</code>.</p>
                <p>Metrics: <code>gld_transactions_per_request</code> (and the equivalent for stores).</p>
                <p>Strided access leads to more global memory transactions per request.</p>
                </section>

                <section><h4>Matrix transpose with shared memory</h4>
                <p><img src="memory_figs/figure_5_15.png"></p>
                <p>The transpose occurs in shared memory. Simple implementation would result in bank conflicts, but this is still faster than uncoalesced global memory access.</p>
                </section>

                <section><h4>Matrix transpose with shared memory</h4>
                <p><pre><code>__global__ void transposeSmem(float *out, float *in, int nx, int ny) {
    // static shared memory
    __shared__ float tile[BDIMY][BDIMX];

    // coordinate in original matrix
    unsigned int ix,iy,ti,to;
    ix = blockIdx.x *blockDim.x + threadIdx.x;
    iy = blockIdx.y *blockDim.y + threadIdx.y;

    // linear global memory index for original matrix
    ti = iy*nx + ix;

    // thread index in transposed block
    unsigned int bidx,irow,icol;
    bidx = threadIdx.y*blockDim.x + threadIdx.x;
    irow = bidx/blockDim.y;
    icol = bidx%blockDim.y;

    // coordinate in transposed matrix
    ix = blockIdx.y * blockDim.y + icol;
    iy = blockIdx.x * blockDim.x + irow;

    // linear global memory index for transposed matrix
    to = iy*ny + ix;

    // transpose with boundary test
    if (ix &lt nx && iy &lt ny)
    {
        // load data from global memory to shared memory
        tile[threadIdx.y][threadIdx.x] = in[ti];

        // thread synchronization
        __syncthreads();

        // store data to global memory from shared memory
        out[to] = tile[icol][irow];
    }
}</code></pre></p>
                </section>

                <section><h4>Matrix transpose with shared memory</h4>
                <p><ol><li>A warp performs a coalesced read of a row from a block of the original matrix in global memory.</li>
                       <li>The warp then writes data into shared memory with row-major ordering (no bank conflicts).</li>
                       <li>Synchronisation of all read/write operations to ensure 2D shared array is filled correctly.</li>
                       <li>The warp reads a column from the 2D sgared array (bank conflicts!)</li>
                       <li>The warp then writes that data into a row of the transposed matrix in global memory (coalesced access).</li></ol></p>
                </section>

                <section><h4>Matrix transpose with shared memory</h4>
                <p>Full code in <code>transposeRectangle.cu</code></p>
                <p>Index calculation is more complex...</p>
                </section>

                <section><h4>Indices used</h4>
                <p><img src="memory_figs/figure_5_16.png"></p>
                </section>

                <section><h4>Matrix transpose with shared memory</h4>
                <p>Kernels may be further optimised with padding (to avoid bank conflicts) and unrolling to improve thread utilisation...</p>
                </section>

                <section><h2>Constant memory</h2></section>

                <section><h3>Constant memory</h3>
                <p>Read-only memory held on DRAM, but with an on-chip cache.</p>
                <p>Limit of 64KB per SM.</p>
                <p>Optimal access pattern is for all threads in a warp to access the same location in constant memory.</p>
                </section>

                <section><h4>Constant memory</h4>
                <p>Constant variables are declared in global scope using <code>__constant__</code>. They exist for the lifetime of the application.</p>
                <p>They must be initialised from host code: <code>cudaError_t cudaMemcpyToSymbol(const void *symbol, const void *src, size_t count, size_t offset, cudaMemcpyKind kind)</code></p>
                </section>

                <section><h4>Finite differencing example</h4>
                <p>We will consider a 1D nine-point finite differencing stencil (e.g. 8th-order first derivative)</p>
                <p><img src="memory_figs/figure_5_18.png"></p>
                <p>$$f'(x) \approx c_0(f(x+4h)-f(x-4h)) + c_1(f(x+3h)-f(x-3h))$$</p>
                <p>$$ - c_2(f(x+2h) - f(x-2h)) + c_3(f(x+h) - f(x-h))$$</p>
                </section>

                <section><h4>Finite differencing example</h4>
                <p>Full code in <code>constantStencil.cu</code></p>
                <p>Constant memory can be used to store the coefficients $c_0$, $c_1$, $c_2$ and $c_3$ as they never change and are identical for all threads.</p>
                </section>

                <section><h4>Finite differencing example</h4>
                <p>We need a "halo" of points on either side of the boundary points to be able to calculate the stencil (these points are more often referred to as "ghost zones").</p>
                <p><img src="memory_figs/figure_5_19.png"></p>
                <p>Shared memory is used to reduce redundant global memory accesses (we reduce 9 points to 1): <code>__shared__ float smem[BDIM + 2 * RADIUS]</code> where <code>RADIUS</code> is the size of each ghost zone.</p>
                </section>

                <section><h4>Finite differencing example</h4>
                <p><pre><code>__global__ void stencil_1d(float *in, float *out) {
    // shared memory
    __shared__ float smem[BDIM + 2*RADIUS];

    // index to global memory
    int idx = threadIdx.x + blockIdx.x * blockDim.x;

    // index to shared memory for stencil calculatioin
    int sidx = threadIdx.x + RADIUS;

    // Read data from global memory into shared memory
    smem[sidx] = in[idx];

    // read halo part to shared memory
    if (threadIdx.x &lt RADIUS) {
        smem[sidx - RADIUS] = in[idx - RADIUS];
        smem[sidx + BDIM] = in[idx + BDIM];
    }

    // Synchronize (ensure all the data is available)
    __syncthreads();

    // Apply the stencil
    float tmp = 0.0f;

    #pragma unroll
    for (int i = 1; i <= RADIUS; i++) {
        tmp += coef[i] * (smem[sidx+i] - smem[sidx-i]);
    }

    // Store the result
    out[idx] = tmp;
}</code></pre></p>
                </section>

                <section><h4>Finite differencing example</h4>
                <p>The declaration of the <code>coef</code> array in constant memory is:</p>
                <p><pre><code>__constant__ float coef[RADIUS + 1];</code></pre></p>
                <p>Initialisation of the values of <code>coef</code> is done with:</p>
                <p><pre><code>void setup_coef_constant(void) {
    const float h_coef[] = {a0, a1, a2, a3, a4};
    cudaMemcpyToSymbol(coef, h_coef, (RADIUS + 1) * sizeof(float));
}</code></pre></p>
                </section>

                <section><h2>Warp shuffle</h2></section>

                <section><h3>Warp shuffle</h3>
                <p><ul><li>Warp shuffles allow threads to read another thread's register (within the same warp).</li>
                       <li>Data can be shared between threads without going through shared or global memory.</li>
                       <li>Latency is lower than shared memory, and no extra memory is consumed for the data exchange.</li></ul></p>
                </section>

                <section><h4>Warp shuffle</h4>
                <p><ul><li>Shuffle instructions refer to <i>lanes</i> within a warp.</li>
                       <li>A lane is simply a single thread within a warp.</li>
                       <li>Each lane has a lane index between 0 and 31.</li></ul></p>
                </section>

                <section><h4>Warp shuffle</h4>
                <p>The lane index and warp index of a thread (in a 1D thread block) can be calculated using:</p>
                <p><pre><code>laneID = threadIdx.x % 32
warpID = threadIdx.x / 32</code></pre></p>
                </section>

                <section><h4>Variants of the warp shuffle instruction</h4>
                <p><ul><li>There are two sets of shuffle instructions: one for integers and one for floats.</li>
                       <li>Each set has four variants of the shuffle instruction.</li>
                       <li>...</li></ul></p>
                </section>

			</div>
		</div>

	<script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.9.0/js/reveal.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.5.0/plugin/math/math.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.9.0/plugin/highlight/highlight.js"></script>
    <script>
      Reveal.initialize({
        hash: true,
        math: {
        mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
        config: 'TeX-AMS_HTML-full',
        // pass other options into `MathJax.Hub.Config()`
        TeX: { Macros: { RR: "{\\bf R}" } }
        },
        plugins: [ RevealHighlight ]
      });
    </script>

	</body>
</html>
