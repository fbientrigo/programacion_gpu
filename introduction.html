
<!DOCTYPE html>
<html>
<head>

<meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="chrome=1" />

<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />


<title>Introduction</title>

<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<!-- General and theme style sheets -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.5.0/css/reveal.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.5.0/css/theme/white.css" id="theme">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.5.0/lib/css/zenburn.css">

<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.5.0/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
}

</script>

<!--[if lt IE 9]>
<script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.5.0/lib/js/html5shiv.js"></script>
<![endif]-->

<!-- Loading the mathjax macro -->
<!-- Load mathjax -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_HTML"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: false }
        }
    });
    </script>
    <!-- End of mathjax configuration -->

<!-- Get Font-awesome from cdn -->
<!--<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.css">-->


        <style type="text/css">
            .container{
                        display: flex;
                      }
            .col{
                      flex: 1;
                }
            .reveal section p {
                      display: inline-block;
                      font-size: 0.6em;
                      line-height: 1.2em;
                      vertical-align: top;
                      text-align: left;
            }
            .reveal section li {
                      font-size: 0.6em;
            }
            .reveal section img {
                      border: none;
            }
        </style>

	</head>
	<body>
		<div class="reveal">
			<div class="slides">

                <section><h1>Optimizaci칩n y Programaci칩n GPU</h1>
                </section>

                <section><h2>Informaci칩n sobre el curso</h2>
                <p><ul><li>Libros:</li>
                        <ul><li>"Professional CUDA C Programming", Cheng, Grossmann, McKercher</li>   
                        <li>"Parallel Programming: Concepts and Practice", Schmidt, Gonzalez-Dominguez, Hundt, Schlarb</li>
                        <li>"Hands On GPU Programming with Python and CUDA", Tuomanen</li></ul>
                       <li>El lenguajes del curso es CUDA/C</li></ul>
                       <!--<li>El horario de la clase es martes 10.15-11.15, miercoles 10.15-11.15.</li>
                       <li>Las evaluaciones ser치n en forma de tareas.</li></ul>-->
                </section>

                <section><h2>Programa</h2>
                <p><ul><li>Introduction and the CUDA programming model</li>
                       <li>CUDA execution model</li>
                       <li>Global memory</li>
                       <li>Shared and constant memory</li>
                       <li>Streams and concurrency</li>
                       <li>Tuning instruction-level primitives</li>
                       <li>CUDA libraries and Python</li>
                       <li>Aplicaciones (Nbody, ray tracing, OpenGL)</li>
                       <li>Proyecto final</li></ul>
                </section>

                <section><h1>Introduction and the CUDA programming model</h1>
                </section>

				<section><h3>Introduction</h3>
                <p>Heterogeneous programming</p>
                <img src="introduction_figs/hetero_arch.png">
                </section>

                <section><h4>GPU Hardware</h4>
                <img src="introduction_figs/fermi_kepler_performance.png">
                </section>
 
                <section><h4>GPU Hardware</h4>
                <img src="introduction_figs/modern_gpu_performance.png">
                </section>

				<section><h4>Compute capability</h4>
                <p><a href="https://developer.nvidia.com/cuda-gpus">NVIDIA GPU Compute Capability</a></p>
                </section>

				<section><h4>CPU vs GPU</h4>
                <img src="introduction_figs/cpu_vs_gpu.png">
                </section>         

				<section><h4>Accelerating a code</h4>
                <img src="introduction_figs/use_of_gpu.png">
                </section>

				<section><h4>GPU thread vs CPU thread</h4>
                <div class="container">
                <div class="col">
                <ul><li class="fragment" data-fragment-index=1>Threads on CPU are heavyweights - OS must swap threads on and off CPU execution channels for multithreading. This <i>context switching</i> is expensive.</li>
                    <li class="fragment" data-fragment-index=3>CPU cores minimise <i>latency</i> for one or two threads.</li>
                    <li class="fragment" data-fragment-index=5>A CPU with 4 quad-core processors can run 16 threads concurrently (32 if hyper-threading is supported).</li></ul>
                </div>
                <div class="col">
                <ul><li class="fragment" data-fragment-index=2>Threads on GPU are lightweights - 1000s of threads available, context switching is fast.</li>
                    <li class="fragment" data-fragment-index=4>GPU cores handle many concurrent lightweight threads to maximise <i>throughput</i>.</li>
                    <li class="fragment" data-fragment-index=6>Modern GPUs support 1536 active threads per mutliprocessor. On a GPU with 16 multiprocessors this means over 24000 concurrently active threads.</li></ul>
                </div>
                </div>
                </section>

                <section><h4>CUDA Platform</h4>
                <img src="introduction_figs/platform.png">
                </section>

                <section><h4>Runtime API vs driver API</h4>
                <img src="introduction_figs/runtime_vs_driver.png">
                <p>Runtime or driver API may be used - no significant difference in performance. Runtime API is easier to work with.</p>
                </section>

                <section><h4>NVCC Compiler</h4>
                <img src="introduction_figs/nvcc_compiler.png">
                <ul><li>Host code: runs on CPU</li>
                    <li>Device code: runs on GPU</li></ul>
                </section>

                <section><h4><i>Hello World</i> with CUDA</h4>
                <pre><code>#include &ltstdio.h&gt

__global__ void helloFromGPU(void)
{
    printf("Hello World from the GPU!\n");
}

int main(){
    printf("Hello World from the CPU!\n");

    helloFromGPU &lt&lt&lt1, 10&gt&gt&gt();
    cudaDeviceReset(); // Clean all resources associated with this device in this process
    return 0;
}</code></pre>
                <p>Compiled with <code>nvcc -arch sm_50 hello_world.cu -o hello_world</code></p>
                </section>

                <section><h4>Basic CUDA program structure</h4>
                <ol><li>Allocate memory on the GPU (device)</li>
                    <li>Copy data from CPU memory (host) to GPU memory</li>
                    <li>Invoke CUDA kernel to perform computation</li>
                    <li>Copy data back from GPU memory to CPU memory</li>
                    <li>Destroy (free) GPU memory</li></ol>
                <p><i>Hello World</i> code only uses step 3...</p>
                </section>

                <section><h3>CUDA programming model</h3>
                <img src="introduction_figs/programming_model.png">
                </section>

                <section><h4>Memory functions</h4>
                <img src="introduction_figs/memory_functions.png">
                <p><code>cudaError_t cudaMalloc ( void** devPtr, size_t size )</code></p>
                <p><code>cudaError_t cudaMemcpy ( void* dst, const void* src, size_t count, cudaMemcpyKind kind )</code></p>
                <p>Latter function is synchronous: host is blocked until transfer is complete.</p>
                </section>

                <section><h4>cudaMemcpy kinds</h4>
                <ul><li><code>cudaMemcpyHostToHost</code></li>
                    <li><code>cudaMemcpyHostToDevice</code></li>
                    <li><code>cudaMemcpyDeviceToHost</code></li>
                    <li><code>cudaMemcpyDeviceToDevice</code></li></ul>
                </section>

                <section><h4>CUDA error types</h4>
                <p>All CUDA function calls (except kernel launches!) return error codes of an enumerated type <code>cudaError_t</code>.</p>
                <p>Error code can be converted to human-readable format with:</p>
                <p><code>char* cudaGetErrorString(cudaError_t error)</code>.</p>
                </section>

                <section><h4>Memory hierarchy</h4>
                <img src="introduction_figs/memory_hierarchy.png" height=300>
                <p>CUDA exposes the memory hierarchy of the GPU to the programmer.</p>
                <ul><li>Global memory: similar to system RAM</li>
                    <li>Shared memory: similar to CPU cache, but CUDA allows direct control of shared memory</li></ul>
                </section>

                <section><h4>First real example: vector addition</h4>
                <div>
                <img src="introduction_figs/vector_addition.png">
                </div><div>
                <p>We start with a pure-host version (i.e. executes on CPU)</p>
                </div>
                </section>

                <section><h4>Host-based array summation</h4>
                <p><pre><code>#include &ltstdlib.h&gt
#include &ltstring.h&gt
#include &lttime.h&gt

void sumArraysOnHost(float *A, float *B, float *C, const int N) {

  for (int idx=0; idx&ltN; idx++) {
    C[idx] = A[idx] + B[idx];
  }
}

void initialData(float *ip,int size) {

  // generate different seed for random number
  time_t t;
  srand((unsigned int) time(&t));
  for (int i=0; i&ltsize; i++) {
    ip[i] = (float)( rand() & 0xFF )/10.0f;
  }

}

int main(int argc, char **argv) {

  int nElem = 1024;
  size_t nBytes = nElem * sizeof(float);
  float *h_A, *h_B, *h_C;

  h_A = (float *)malloc(nBytes);
  h_B = (float *)malloc(nBytes);
  h_C = (float *)malloc(nBytes);

  initialData(h_A, nElem);
  initialData(h_B, nElem);

  sumArraysOnHost(h_A, h_B, h_C, nElem);

  free(h_A);
  free(h_B);
  free(h_C);

  return(0);

}</code></pre></p>
                </section>

                <section><h4>Using the GPU</h4>
                <p>Now we will modify the code to use the GPU. First we must declare memory on the GPU to store the arrays.</p>
                <p><pre><code>float *d_A, *d_B, *d_C;
cudaMalloc((float**)&d_A, nBytes);
cudaMalloc((float**)&d_A, nBytes);
cudaMalloc((float**)&d_A, nBytes);</code></pre></p>
                <p>Transferring the data from the host to the device:</p>
<p><pre><code>float *d_A, *d_B, *d_C;
cudaMemcpy(d_A, h_A, nBytes, cudaMemcpyHostToDevice);
cudaMemcpy(d_B, h_B, nBytes, cudaMemcpyHostToDevice);</code></pre></p>
                </section>

                <section><h4>Using the GPU</h4>
                <p>A kernel would now be invoked (we will write the kernel later) to perform the calculation. As the kernel executes the host is NOT blocked. When the calculation is complete the result is on the GPU in <code>d_C</code>. This must be copied back to the host (to an array called <code>gpuRef</code>):</p>
                <p><code>cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost);</code></p>
                <p>This call blocks the host. Memory on the GPU is then released:</p>
                <p><pre><code>cudaFree(d_A);
cudaFree(d_B);
cudaFree(d_C);</code></pre></p>
                </section>

                <section><h4>Thread organisation</h4>
                <img src="introduction_figs/threads_blocks_grid.png" height=300>
                <ul><li>All threads in a kernel launch are called a <i>grid</i> and share a global memory space.</li>
                    <li>A grid is composed of <i>thread blocks</i>. These have block-local synchronisation and shared memory.</li>
                    <li>Threads have unique coordinates: <code>blockIdx</code> (block index within a grid) and <code>threadIdx</code> (thread index within a block).</li></ul>
                </section>

                <section><h4>Thread organisation</h4>
                <p>Coordinates are a vector type <code>uint3</code> (device side) defined in CUDA:</p>
                <p><code>blockIdx.x</code>, <code>blockIdx.y</code>, <code>blockIdx.z</code><p>
                <p><code>threadIdx.x</code>, <code>threadIdx.y</code>, <code>threadIdx.z</code><p>
                <p>Grid and block dimensions are specified using built-in variables of type <code>dim3</code> (host side):</p>
                <p><code>blockDim</code> (measured in threads), <code>gridDim</code> (measured in blocks)</p>
                </section>

                <section><h4>Thread organisation: code example 1</h4>
                <p><pre><code>#include &ltcuda_runtime.h&gt
#include &ltstdio.h&gt

__global__ void checkIndex(void) {
    printf("threadIdx:(%d, %d, %d) blockIdx:(%d, %d, %d) blockDim:(%d, %d, %d) "
        "gridDim:(%d, %d, %d)\n", threadIdx.x, threadIdx.y, threadIdx.z,
        blockIdx.x, blockIdx.y, blockIdx.z, blockDim.x, blockDim.y, blockDim.z,
        gridDim.x,gridDim.y,gridDim.z);
}

int main(int argc, char **argv) {
  // define total data element
  int nElem = 6;
 
  // define grid and block structure
  dim3 block (3);
  dim3 grid ((nElem+block.x-1)/block.x);

  // check grid and block dimension from host side
  printf("grid.x %d grid.y %d grid.z %d\n",grid.x, grid.y, grid.z);
  printf("block.x %d block.y %d block.z %d\n",block.x, block.y, block.z);

  // check grid and block dimension from device side
  checkIndex &lt&lt&ltgrid, block&gt&gt&gt ();

  // reset device before you leave
  cudaDeviceReset();

  return(0);
}</code></pre></p>
                <p>Compile with <code>nvcc -arch=sm_50 checkDimension.cu -o checkDimension</code></p>
                </section>

                <section><h4>Thread organisation</h4>
                <p>For a given problem we must:</p>
                <p><ul><li>Decide the block size</li>
                       <li>Calculate the grid dimension based on data size and block size</li></ul></p>
                <p>To determine the block size we must consider:</p>
                <p><ul><li>Performance characteristics of the kernel</li>
                       <li>Limitations on GPU resources</li></ul></p>
                </section>

                <section><h4>Thread organisation: code example 2</h4>
                <p><pre><code>#include &ltcuda_runtime.h&gt
#include &ltstdio.h&gt

int main(int argc, char **argv) {
  // define total data elements
  int nElem = 1024;

  // define grid and block structure
  dim3 block (1024);
  dim3 grid ((nElem+block.x-1)/block.x);
  printf("grid.x %d block.x %d \n",grid.x, block.x);

  // reset block
  block.x = 512;
  grid.x = (nElem+block.x-1)/block.x;
  printf("grid.x %d block.x %d \n",grid.x, block.x);

  // reset block
  block.x = 256;
  grid.x = (nElem+block.x-1)/block.x;
  printf("grid.x %d block.x %d \n",grid.x, block.x);

  // reset block
  block.x = 128;
  grid.x = (nElem+block.x-1)/block.x;
  printf("grid.x %d block.x %d \n",grid.x, block.x);

  // reset device before you leave
  cudaDeviceReset();
  return(0);
}</code></pre></p>
                <p>Compile with: <code>nvcc defineGridBlock.cu -o defineGridBlock</code></p>
                </section>

                <section><h3>Launching kernels</h3>
                <p><code>kernel_name &lt&lt&ltgrid, block&gt&gt&gt(argument list);</code></p>
                <p>Note that threads within a block share memory (they can communicate). Threads in different blocks <i>cannot</i> communicate.</p>
                <p>Example: 32 data elements, 8 elements per block: <code>kernel_name&lt&lt&lt4, 8&gt&gt&gt(argument list);</code></p>
                <p><img src="introduction_figs/thread_layout.png"></p>
                </section>

                <section><h3>Writing kernels</h3>
                <ul><li>Kernels are based on the SPMD model (single program multiple data)</li>
                    <li>A kernel is scalar code for a single thread, including the data access for that thread.</li>
                    <li>When the kernel is called many threads perform the same operation as defined in the kernel.</li>
                    <li>A kernel is defined using the <code>__global__</code> declaration specification:</li></ul>
                <p><code>__global__ void kernel_name(argument list);</code></p>
                <p>A kernel function must have a <code>void</code> return type.</p>
                </section>

                <section><h4>Function type qualifiers</h4>
                <p><img src="introduction_figs/function_qualifiers.png"></p>
                <p>A function can be compiled for both host and device using the relevant qualifiers together.</p>
                </section>

                <section><h4>Kernel restrictions</h4>
                <p><img src="introduction_figs/kernel_restrictions.png"></p>
                </section>

                <section><h4>Example: host vector addition</h4>
                <p><pre><code>void sumArraysOnHost(float *A, float *B, float *C, const int N) {
  for (int i = 0; i &lt N; i++)
    C[i] = A[i] + B[i];
}</code></pre></p>
                </section>

                <section><h4>Example: device vector addition</h4>
                <p><pre><code>__global__ void sumArraysOnGPU(float *A, float *B, float *C) {
  int i = threadIdx.x;
  C[i] = A[i] + B[i];
}</code></pre></p>
                <p>There is no <code>for</code> loop, the thread coordinates replace the array index and <code>N</code> is implicitly defined by launching <code>N</code> threads.</p>
                <p>For 32 elements, this kernel may be invoked as: <code>sumArraysOnGPU&lt&lt&lt1,32&gt&gt&gt(float *A, float *B, float *C);</code></p>
                </section>

                <section><h4>Example: verifying the kernel</h4>
                <p><pre><code>void checkResult(float *hostRef, float *gpuRef, const int N) {

  double epsilon = 1.0E-8;
  int match = 1;
  for (int i = 0; i &lt N; i++) {
    if (abs(hostRef[i] - gpuRef[i]) &gt epsilon) {
      match = 0;
      printf("Arrays do not match!\n");
      printf("host %5.2f gpu %5.2f at current %d\n",hostRef[i], gpuRef[i], i);
      break;
    }
  }

  if (match) printf("Arrays match.\n\n");
  return;
}</code></pre></p>
                </section>

                <section><h3>Handling errors</h3>
                <p>We can define an error-handling macro to wrap the CUDA API calls:</p>
                <p><pre><code>#define CHECK(call)                        \
{                                  \
  const cudaError_t error = call;  \
  if (error != cudaSuccess)        \
  {                                \
    printf("Error: %s:%d, ", __FILE__, __LINE__);  \
    printf("code:%d, reason: %s\n", error, cudaGetErrorString(error));  \
    exit(1);  \
  }           \
}</code></pre></p>
                </section>

                <section><h4>Handling errors</h4>
                <p>This macro can then be used as, for example:</p>
                <p><code>CHECK(cudaMemcpy(d_C, gpuRef, nBytes, cudaMemcpyHostToDevice));</code></p>
                <p>It is often very useful to check for kernel errors during debugging:</p>
                <p><code>kernel_function&lt&lt&ltgrid, block&gt&gt&gt(argument list);
CHECK(cudaDeviceSynchronize());</code></p>
                </section>

                <section><h4>Full vector addition code</h4>
                <p><pre><code>#include &ltcuda_runtime.h&gt
#include &ltstdio.h&gt

#define CHECK(call)               \
{                                 \
  const cudaError_t error = call; \
  if (error != cudaSuccess)       \
  {                               \
    printf("Error: %s:%d, ", __FILE__, __LINE__);  \
    printf("code:%d, reason: %s\n", error, cudaGetErrorString(error));  \
    exit(1);                      \
  }                               \
}

void checkResult(float *hostRef, float *gpuRef, const int N) {
  double epsilon = 1.0E-8;
  bool match = 1;
  for (int i=0; i &lt N; i++) {
    if (abs(hostRef[i] - gpuRef[i]) &gt epsilon) {
      match = 0;
      printf("Arrays do not match!\n");
      printf("host %5.2f gpu %5.2f at current %d\n",hostRef[i],gpuRef[i],i);
      break;
    }
  }

  if (match) printf("Arrays match.\n\n");
}

void initialData(float *ip,int size) {
  // generate different seed for random number
  time_t t;
  srand((unsigned) time(&t));

  for (int i=0; i &lt size; i++) {
    ip[i] = (float)( rand() & 0xFF )/10.0f;
  }
}

void sumArraysOnHost(float *A, float *B, float *C, const int N) {
  for (int idx=0; idx &lt N; idx++)
    C[idx] = A[idx] + B[idx];
}

__global__ void sumArraysOnGPU(float *A, float *B, float *C) {
  int i = threadIdx.x;
  C[i] = A[i] + B[i];
}

int main(int argc, char **argv) {
  printf("%s Starting...\n", argv[0]);

  // set up device
  int dev = 0;
  cudaSetDevice(dev);

  // set up data size of vectors
  int nElem = 32;
  printf("Vector size %d\n", nElem);

  // malloc host memory
  size_t nBytes = nElem * sizeof(float);

  float *h_A, *h_B, *hostRef, *gpuRef;

  h_A = (float *)malloc(nBytes);
  h_B = (float *)malloc(nBytes);

  hostRef = (float *)malloc(nBytes);
  gpuRef = (float *)malloc(nBytes);

  // initialize data at host side
  initialData(h_A, nElem);
  initialData(h_B, nElem);

  memset(hostRef, 0, nBytes);
  memset(gpuRef, 0, nBytes);

  // malloc device global memory
  float *d_A, *d_B, *d_C;

  cudaMalloc((float**)&d_A, nBytes);
  cudaMalloc((float**)&d_B, nBytes);
  cudaMalloc((float**)&d_C, nBytes);

  // transfer data from host to device
  cudaMemcpy(d_A, h_A, nBytes, cudaMemcpyHostToDevice);
  cudaMemcpy(d_B, h_B, nBytes, cudaMemcpyHostToDevice);

  // invoke kernel at host side
  dim3 block (nElem);
  dim3 grid (nElem/block.x);

  sumArraysOnGPU &lt&lt&ltgrid, block&gt&gt&gt (d_A, d_B, d_C);
  printf("Execution configuration &lt&lt&lt%d, %d&gt&gt&gt\n",grid.x,block.x);

  // copy kernel result back to host side
  cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost);

  // add vector at host side for result checks
  sumArraysOnHost(h_A, h_B, hostRef, nElem);

  // check device results
  checkResult(hostRef, gpuRef, nElem);

  // free device global memory
  cudaFree(d_A);
  cudaFree(d_B);
  cudaFree(d_C);

  // free host memory
  free(h_A);
  free(h_B);
  free(hostRef);
  free(gpuRef);

  return(0);
}</code></pre></p>
                </section>

                <section><h4>General indexing</h4>
                <p>The kernel currently uses only thread indexing, assuming just one block is present. We can generalise to use any number of blocks:</p>
                <p><code>__global__ void sumArraysOnGPU(float *A, float *B, float *C) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  C[i] = A[i] + B[i];
}</code></p>
                </section>

                <section><h3>Profiling with nvprof</h3>
                <ul><li>A <i>profiler</i> gives information about the execution of a code (timing, memory usage, etc.)</li>
                    <li>Profiling tools exist for CPU code in C (gprof)</li>
                    <li>For CUDA code there is <i>nvprof</i>. There are more sophisticated visual profilers available as well (nvvp, Nsight).</li></ul>
                <p><code>nvprof [nvprof_args] &ltapplication&gt [application_args]</code></p>
                <p>Useful help is found with: <code>nvprof --help</code></p>
                </section>

                <section><h4>Profiling with nvprof</h4>
                <p>We can get useful information about our program using <code>nvprof ./sumArraysOnGPU</code></p>
                </section>

                <section><h4>Compute or memory bound?</h4>
                <p><img src="introduction_figs/performance1.png" width=600></p>
                <p><img src="introduction_figs/performance2.png" width=600></p>
                </section>

                <section><h3>Organising threads</h3>
                <p>Matrices are typically stored in global memory in row-major order.</p>
                <p><img src="introduction_figs/matrix_linear.png"></p>
                </section>

                <section><h4>Organising threads</h4>
                <p>Thread/block indices can be mapped to matrix indices.</p>
                <p><img src="introduction_figs/matrix_2d.png"></p>
                </section>

                <section><h4>Organising threads</h4>
                <p>Example of relationship between threads, blocks and matrix coordinates.</p>
                <p><img src="introduction_figs/matrix_indices.png"></p>
                </section>

                <section><h4>Code example</h4>
                <p><pre><code>#include &ltcuda_runtime.h&gt
#include &ltstdio.h&gt

#define CHECK(call)     \
{                       \
  const cudaError_t error = call;   \
  if (error != cudaSuccess)         \
  {                                 \
    printf("Error: %s:%d, ", __FILE__, __LINE__);  \
    printf("code:%d, reason: %s\n", error, cudaGetErrorString(error));  \
    exit(-10*error);  \
  }                   \
}

void initialInt(int *ip, int size) {
  for (int i=0; i &lt size; i++) {
    ip[i] = i;
  }
}

void printMatrix(int *C, const int nx, const int ny) {
  int *ic = C;
  printf("\nMatrix: (%d.%d)\n",nx,ny);

  for (int iy=0; iy &lt ny; iy++) {
    for (int ix=0; ix &lt nx; ix++) {
      printf("%3d",ic[ix]);
    }
    ic += nx;
    printf("\n");
  }
  printf("\n");
}

__global__ void printThreadIndex(int *A, const int nx, const int ny) {

  int ix = threadIdx.x + blockIdx.x * blockDim.x;
  int iy = threadIdx.y + blockIdx.y * blockDim.y;
  unsigned int idx = iy*nx + ix;

  printf("thread_id (%d,%d) block_id (%d,%d) coordinate (%d,%d) "
      "global index %2d ival %2d\n", threadIdx.x, threadIdx.y, blockIdx.x,
      blockIdx.y, ix, iy, idx, A[idx]);
}

int main(int argc, char **argv) {
  printf("%s Starting...\n", argv[0]);

  // get device information
  int dev = 0;
  cudaDeviceProp deviceProp;
  CHECK(cudaGetDeviceProperties(&deviceProp, dev));
  printf("Using Device %d: %s\n", dev, deviceProp.name);
  CHECK(cudaSetDevice(dev));

  // set matrix dimension
  int nx = 8;
  int ny = 6;
  int nxy = nx*ny;
  int nBytes = nxy * sizeof(float);

  // malloc host memory
  int *h_A;
  h_A = (int *)malloc(nBytes);

  // iniitialize host matrix with integer
  initialInt(h_A, nxy);
  printMatrix(h_A, nx, ny);

  // malloc device memory
  int *d_MatA;
  cudaMalloc((void **)&d_MatA, nBytes);

  // transfer data from host to device
  cudaMemcpy(d_MatA, h_A, nBytes, cudaMemcpyHostToDevice);

  // set up execution configuration
  dim3 block(4, 2);
  dim3 grid((nx+block.x-1)/block.x, (ny+block.y-1)/block.y);

  // invoke the kernel
  printThreadIndex &lt&lt&lt grid, block &gt&gt&gt(d_MatA, nx, ny);
  cudaDeviceSynchronize();

  // free host and devide memory
  cudaFree(d_MatA);
  free(h_A);

  // reset device
  cudaDeviceReset();
  return (0);
}</code></pre></p>
                </section>

                <section><h3>Summing matrices with a 2D grid and 2D blocks</h3>
                <p>Host version:</p>
                <p><pre><code>void sumMatrixOnHost (float *A, float *B, float *C, const int nx, const int ny) {
  float *ia = A;
  float *ib = B;
  float *ic = C;

  for (int iy=0; iy &lt ny; iy++) {
    for (int ix=0; ix &lt nx; ix++) {
      ic[ix] = ia[ix] + ib[ix];
    }
    ia += nx; ib += nx; ic += nx;
  }
}</code></pre></p>
                </section>

                <section><h4>Summing matrices with a 2D grid and 2D blocks</h4>
                <p>Device version:</p>
                <p><pre><code>__global__ void sumMatrixOnGPU2D(float *MatA, float *MatB, float *MatC, int nx, int ny) {

  unsigned int ix = threadIdx.x + blockIdx.x * blockDim.x;
  unsigned int iy = threadIdx.y + blockIdx.y * blockDim.y;
  unsigned int idx = iy*nx + ix;

  if (ix &lt nx && iy &lt ny)
    MatC[idx] = MatA[idx] + MatB[idx];
}</code></pre></p>
                </section>

                <section><h4>Summing matrices with a 2D grid and 2D blocks</h4>
                <p>Key to the kernel is the map from thread index to global linear memory index.</p>
                <p><img src="introduction_figs/matrix_mapping.png"></p>
                </section>

                <section><h4>Summing matrices with a 2D grid and 2D blocks</h4>
                <p>We will use $16384$ elements per dimension, that is <code>1 &lt&lt 14</code>.</p>
                <p>Kernel configuration is determined as:</p>
                <p><code>int dimx = 32;
int dimy = 32;
dim3 block(dimx, dimy);
dim3 grid((nx + block.x - 1) / block.x, (ny + block.y - 1) / block.y);</code></p>
                <p>Note that the blocks have $32 \times 32 = 1024$ threads.</p>
                </section>

                <section><h4>Summing matrices with a 2D grid and 2D blocks (partial code)</h4>
                <p><pre><code>#include &ltcuda_runtime.h&gt
#include &ltstdio.h&gt
#include &ltsys/time.h&gt

double cpuSecond() {
  struct timeval tp;
  gettimeofday(&tp,NULL);
  return ((double)tp.tv_sec + (double)tp.tv_usec*1.e-6);
}

int main(int argc, char **argv) {
  printf("%s Starting...\n", argv[0]);

  // set up device
  int dev = 0;
  cudaDeviceProp deviceProp;
  CHECK(cudaGetDeviceProperties(&deviceProp, dev));
  printf("Using Device %d: %s\n", dev, deviceProp.name);
  CHECK(cudaSetDevice(dev));

  // set up date size of matrix
  int nx = 1 &lt&lt 14;
  int ny = 1 &lt&lt 14;
  int nxy = nx*ny;
  int nBytes = nxy * sizeof(float);
  printf("Matrix size: nx %d ny %d\n",nx, ny);

  // malloc host memory
  float *h_A, *h_B, *hostRef, *gpuRef;
  h_A = (float *)malloc(nBytes);
  h_B = (float *)malloc(nBytes);
  hostRef = (float *)malloc(nBytes);
  gpuRef = (float *)malloc(nBytes);

  // initialize data at host side
  double iStart = cpuSecond();
  initialData (h_A, nxy);
  initialData (h_B, nxy);
  double iElaps = cpuSecond() - iStart;

  memset(hostRef, 0, nBytes);
  memset(gpuRef, 0, nBytes);

  // add matrix at host side for result checks
  iStart = cpuSecond();
  sumMatrixOnHost (h_A, h_B, hostRef, nx,ny);
  iElaps = cpuSecond() - iStart;

  // malloc device global memory
  float *d_MatA, *d_MatB, *d_MatC;
  cudaMalloc((void **)&d_MatA, nBytes);
  cudaMalloc((void **)&d_MatB, nBytes);
  cudaMalloc((void **)&d_MatC, nBytes);

  // transfer data from host to device
  cudaMemcpy(d_MatA, h_A, nBytes, cudaMemcpyHostToDevice);
  cudaMemcpy(d_MatB, h_B, nBytes, cudaMemcpyHostToDevice);

  // invoke kernel at host side
  int dimx = 32;
  int dimy = 32;
  dim3 block(dimx, dimy);
  dim3 grid((nx+block.x-1)/block.x, (ny+block.y-1)/block.y);

  iStart = cpuSecond();
  sumMatrixOnGPU2D &lt&lt&lt grid, block &gt&gt&gt(d_MatA, d_MatB, d_MatC, nx, ny);
  cudaDeviceSynchronize();
  iElaps = cpuSecond() - iStart;
  printf("sumMatrixOnGPU2D &lt&lt&lt(%d,%d), (%d,%d)&gt&gt&gt elapsed %f sec\n", grid.x, grid.y, block.x, block.y, iElaps);

  // copy kernel result back to host side
  cudaMemcpy(gpuRef, d_MatC, nBytes, cudaMemcpyDeviceToHost);

  // check device results
  checkResult(hostRef, gpuRef, nxy);

  // free device global memory
  cudaFree(d_MatA);
  cudaFree(d_MatB);
  cudaFree(d_MatC);

  // free host memory
  free(h_A);
  free(h_B);
  free(hostRef);
  free(gpuRef);

  // reset device
  cudaDeviceReset();
  return (0);
}</code></pre></p>
                </section>

                <section><h4>Summing matrices with a 2D grid and 2D blocks</h4>
                <p><img src="introduction_figs/matrix_timings.png"></p>
                </section>

                <section><h4>Summing matrices with a 1D grid and 1D blocks</h4>
                <p><img src="introduction_figs/matrix_1d_blocks.png"></p>
                </section>

                <section><h4>Summing matrices with a 1D grid and 1D blocks</h4>
                <p><pre><code>__global__ void sumMatrixOnGPU1D(float *MatA, float *MatB, float *MatC, int nx, int ny) {
  unsigned int ix = threadIdx.x + blockIdx.x * blockDim.x;
  if (ix &lt nx ) {
    for (int iy=0; iy &lt ny; iy++) {
      int idx = iy*nx + ix;
      MatC[idx] = MatA[idx] + MatB[idx];
    }
  }
}</code></pre></p>
                </section>

                <section><h4>Summing matrices with a 1D grid and 1D blocks</h4>
                <p>The 1D grid and block configuration is set as follows:</p>
                <p><pre><code>dim3 block(32,1);
dim3 grid((nx+block.x-1)/block.x,1);</code></pre></p>
                <p>The kernel is then invoked as:</p>
                <p><pre><code>sumMatrixOnGPU1D &lt&lt&lt grid, block &gt&gt&gt(d_MatA, d_MatB, d_MatC, nx, ny);</code></pre></p>
                </section>

                <section><h4>Summing matrices with a 1D grid and 1D blocks</h4>
                <p>Performance is similar to 2D grid with block ($32 \times 32$) configuration.</p>
                <p>Increasing the block size (to 128) leads to a faster kernel...</p>
                </section>

                <section><h4>Summing matrices with a 2D grid and 1D blocks</h4>
                <p><img src="introduction_figs/matrix_2dgrid_1dblock.png"></p>
                <p>This is a special case of 2D grid, 2D block, where the second dimension of the blocks is equal to $1$.</p>
                </section>

                <section><h4>Summing matrices with a 2D grid and 1D blocks</h4>
                <p><pre><code>ix = threadIdx.x + blockIdx.x * blockDim.x;
iy = blockIdx.y;</code></pre></p>
                </section>

                <section><h4>Summing matrices with a 2D grid and 1D blocks</h4>
                <p>New kernel:</p>
                <p><pre><code>__global__ void sumMatrixOnGPUMix(float *MatA, float *MatB, float *MatC, int nx, int ny) {
  unsigned int ix = threadIdx.x + blockIdx.x * blockDim.x;
  unsigned int iy = blockIdx.y;
  unsigned int idx = iy*nx + ix;
  if (ix &lt nx && iy &lt ny)
    MatC[idx] = MatA[idx] + MatB[idx];
}</code></pre></p>
                </section>

                <section><h4>Summing matrices with a 2D grid and 1D blocks</h4>
                <p><ul><li>The 2D kernel sumMatrixOnGPU2D also works for this execution configuration.</li>
                       <li>The only advantage of using this new kernel is to eliminate one integer multiplication and one integer addition per thread.</li>
                       <li>We set the block dimension to $32$ and calculate the grid size:</li></ul></p>
                       <p><pre><code>dim3 block(32);
dim3 grid((nx + block.x - 1) / block.x,ny);</code></pre></p>
                       <p>Kernel invocation:</p>
                       <p><pre><code>sumMatrixOnGPUMix &lt&lt&lt grid, block &gt&gt&gt(d_MatA, d_MatB, d_MatC, nx, ny);</code></pre></p>
                       <p>Compile with: <code>nvcc -arch=sm_50 sumMatrixOnGPU-2D-grid-1D-block.cu -o mat2D1D</code></p>
                </section>

                <section><h4>Timings of different kernel implementations</h4>
                <p><img src="introduction_figs/kernel_timings.png"></p>
                </section>

                <section><h4>Finding information about GPUs on the system</h4>
                <p><ul><li>With the runtime API: <code>cudaGetDeviceProperties</code>. See code in CUDA Toolkit <i>samples</i> directory, <code>deviceQuery</code>.</li>
                       <li>On the command line: <code>nvidia-smi</code></li></ul></p>
                </section>

			</div>
		</div>

<script>

require(
    {
      // it makes sense to wait a little bit when you are loading
      // reveal from a cdn in a slow connection environment
      waitSeconds: 15
    },
    [
      "https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.5.0/lib/js/head.min.js",
      "https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.5.0/js/reveal.js"
    ],

    function(head, Reveal){

        // Full list of configuration options available here: https://github.com/hakimel/reveal.js#configuration
        Reveal.initialize({
            controls: true,
            progress: true,
            history: true,

            transition: "slide",

            // Optional libraries used to extend on reveal.js
            dependencies: [
                { src: "https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.5.0/plugin/highlight/highlight.js" }
            ]
        });

        var update = function(event){
          if(MathJax.Hub.getAllJax(Reveal.getCurrentSlide())){
            MathJax.Hub.Rerender(Reveal.getCurrentSlide());
          }
        };

        Reveal.addEventListener('slidechanged', update);

        function setScrollingSlide() {
            var scroll = false
            if (scroll === true) {
              var h = $('.reveal').height() * 0.95;
              $('section.present').find('section')
                .filter(function() {
                  return $(this).height() > h;
                })
                .css('height', 'calc(95vh)')
                .css('overflow-y', 'scroll')
                .css('margin-top', '20px');
            }
        }

        // check and set the scrolling slide every time the slide change
        Reveal.addEventListener('slidechanged', setScrollingSlide);

    }

);
</script>
	</body>
</html>
